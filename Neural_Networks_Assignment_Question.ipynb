{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalajiGurusala/AI/blob/main/Neural_Networks_Assignment_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST dataset, simple fully connected neural network with 3 layers"
      ],
      "metadata": {
        "id": "nmNxXpQjoyPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective is to set up a neural network problem using the MNIST dataset (https://www.tensorflow.org/datasets/catalog/mnist), which is commonly used for classifying handwritten digits. We'll implement a fully connected (FC) neural network and test three different combinations of activation functions, optimizers, regularizers, and loss functions.\n",
        "\n",
        "Here's the plan for three different models:\n",
        "\n",
        "Dataset MNIST provides input of size 28x28.\n",
        "\n",
        "Let's use dense layers of size 128, 64 and 10 (3 layers) for all the 3 models.\n",
        "1. Model 1: Basic Model\n",
        "Activation Function: ReLU for hidden layers, Softmax for output\n",
        "Optimizer: Stochastic Gradient Descent (SGD)\n",
        "Regularizer: None\n",
        "Loss Function: Categorical Crossentropy\n",
        "2. Model 2: Intermediate Model\n",
        "Activation Function: Leaky ReLU for hidden layers, Softmax for output\n",
        "Optimizer: Adam\n",
        "Regularizer: L2 regularization\n",
        "Loss Function: Categorical Crossentropy\n",
        "3. Model 3: Advanced Model\n",
        "Activation Function: Swish for hidden layers, Softmax for output\n",
        "Optimizer: Adam with learning rate decay\n",
        "Regularizer: Dropout\n",
        "Loss Function: Categorical Crossentropy\n",
        "\n",
        "Steps:\n",
        "Data Preparation: We'll load and preprocess the MNIST dataset.\n",
        "Model Definition: Three models will be created with the configurations mentioned above.\n",
        "Training: Train the models and track metrics like loss and accuracy.\n",
        "Evaluation: Compare the performance of the three models\n",
        "\n",
        "MNIST is a simple dataset. The expectation is that though all 3 models might produce a high accuracy, model 3 might perform better than model 2, and model 2 might perform better than model 1. (It need not be true all the time, due to the simple nature of this dataset)"
      ],
      "metadata": {
        "id": "cswr_RSqnK9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxLpzbwudNXe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PklXOOtkLToJ",
        "outputId": "02bffd5b-7606-4888-fc1d-3d70613e38bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "LIGsCW40LRrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uvv3Ak70nJ5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "BKaBCFn4Qz2O",
        "outputId": "e3d02a2b-a3cf-4e5b-dc34-01ef50029906"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2512218947.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_1():\n",
        "    model = Sequential([\n",
        "        # INSERT LAYERS HERE BASED ON THE SPECIFICATIONS MENTIONED ABOVE\n",
        "    ])\n",
        "    # Compile the model using the function model.compile using the optimizer mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "R0GbCClpdmgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_2():\n",
        "    model = Sequential([\n",
        "        # INSERT LAYERS HERE BASED ON THE SPECIFICATIONS MENTIONED ABOVE\n",
        "    ])\n",
        "    # Compile the model using the function model.compile using the optimizer mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "g2VnxXlMdoyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_3():\n",
        "    model = Sequential([\n",
        "        # INSERT LAYERS HERE BASED ON THE SPECIFICATIONS MENTIONED ABOVE\n",
        "    ])\n",
        "    # Compile the model using the function model.compile using the optimizer mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "IyiWLOdwdqOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_plot(model, x_train, y_train, x_test, y_test, epochs=20, batch_size=64):\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Plot the accuracy and loss\n",
        "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['loss'], label='train_loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy / Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}, Test loss: {loss:.4f}\")\n",
        "\n",
        "# Train and plot for all models\n",
        "print(\"Model 1 (Basic Model):\")\n",
        "model_1 = create_model_1()\n",
        "train_and_plot(model_1, x_train, y_train, x_test, y_test)\n",
        "\n",
        "print(\"\\nModel 2 (Intermediate Model):\")\n",
        "model_2 = create_model_2()\n",
        "train_and_plot(model_2, x_train, y_train, x_test, y_test)\n",
        "\n",
        "print(\"\\nModel 3 (Advanced Model):\")\n",
        "model_3 = create_model_3()\n",
        "train_and_plot(model_3, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "k7wRz-ZOdsOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 with similar network"
      ],
      "metadata": {
        "id": "nh5FgU4Lo4yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at a more complex dataset and let's see whether it can be addressed using the similar neural network(s).\n",
        "\n",
        "CIFAR-10 consists of 60,000 color images in 10 classes, with 6,000 images per class. The CIFAR-10 dataset is a more challenging classification problem than MNIST due to the diversity and complexity of the images."
      ],
      "metadata": {
        "id": "uUGm7gLppE8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When working with the MNIST dataset, the images are 28x28 grayscale images (i.e., single-channel). In a basic fully connected (FC) network, you flatten the 2D image (28x28) into a 1D vector of 784 elements before passing it to the dense layers.\n",
        "* However, if we don't explicitly add the Flatten() layer in some frameworks (like TensorFlow), it may be inferred implicitly, especially for simple datasets like MNIST because the data is 2D (28x28). So, you might not have noticed the need for Flatten() in MNIST if it's handled automatically.\n",
        "\n",
        "\n",
        "\n",
        "* In the case of CIFAR-10, the images are 32x32 with 3 color channels (RGB), meaning the shape of each image is (32, 32, 3). Before passing this 3D image to fully connected layers, you need to explicitly flatten the image into a 1D vector of 32*32*3 = 3,072 elements.\n",
        "\n",
        "* If you don’t flatten the image in CIFAR-10, the fully connected layer won’t know how to process the multi-dimensional (3D) image data directly. Unlike in the MNIST case, it’s less likely that this will be inferred automatically.\n",
        "\n",
        "\n",
        "So our first layer in this scenario can be tf.keras.layers.Flatten(input_shape=(32, 32, 3)). This can be followed by fully connected layers of size 512, 256 and 10 respectively"
      ],
      "metadata": {
        "id": "CHDDschAp9_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario too, create 3 models as per the configuration mentioned above and let's compare them"
      ],
      "metadata": {
        "id": "WwsuILQSqec8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert class labels to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "QFhOeSENg-Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FiUY0mhp9mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_1():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Insert layers here as per requirement mentioned above\n",
        "    ])\n",
        "    # Compile the model as per requirement mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "xLLOChyMhCZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_2():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Insert layers here as per requirement mentioned above\n",
        "    ])\n",
        "    # Compile the model as per requirement mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "X6Qb_CMvhWbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_3():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Insert layers here as per requirement mentioned above\n",
        "    ])\n",
        "    # Compile the model as per requirement mentioned above\n",
        "    return model"
      ],
      "metadata": {
        "id": "ux0ddno7hXJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_plot(model, x_train, y_train, x_test, y_test, epochs=20, batch_size=64):\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Plot accuracy and loss\n",
        "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['loss'], label='train_loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy / Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}, Test loss: {loss:.4f}\")\n",
        "\n",
        "# Train and plot for all models\n",
        "print(\"Model 1 (Basic Model):\")\n",
        "model_1 = create_model_1()\n",
        "train_and_plot(model_1, x_train, y_train, x_test, y_test)\n",
        "\n",
        "print(\"\\nModel 2 (Intermediate Model):\")\n",
        "model_2 = create_model_2()\n",
        "train_and_plot(model_2, x_train, y_train, x_test, y_test)\n",
        "\n",
        "print(\"\\nModel 3 (Advanced Model):\")\n",
        "model_3 = create_model_3()\n",
        "train_and_plot(model_3, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "gr0UD-HShZ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is your observation here?\n",
        "\n",
        "Was this the anticipated observation?\n",
        "\n",
        "Try reading up a little and try to identify potential reasons as to why this behaviour was exhibited."
      ],
      "metadata": {
        "id": "11ZL-ogKq6qR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISCELLANEOUS:"
      ],
      "metadata": {
        "id": "fKEgslgJrY-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can you implement a model which solves any issue(s) you encountered here?"
      ],
      "metadata": {
        "id": "PpgQy9F-rOQY"
      }
    }
  ]
}