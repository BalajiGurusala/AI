{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ShopTalk – Image Captioning Pipeline (BLIP & CLIP)\n",
        "\n",
        "**Project:** ShopTalk – AI-Powered Shopping Assistant  \n",
        "**Dataset:** [Amazon Berkeley Objects (ABO)](https://amazon-berkeley-objects.s3.amazonaws.com/index.html)  \n",
        "**Author:** Balaji Gurusala  \n",
        "**Notebook Scope:** T000c from `.spec/tasks.md`  \n",
        "**Prerequisite:** `01-shoptalk-eda.ipynb` must have been run (produces `shoptalk_abo_cleaned.pkl`)  \n",
        "**Environment:** Kaggle GPU (T4/P100) recommended; also works on Mac (MPS) or CPU (slower)\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The problem statement requires **Image Captioning** to augment product descriptions with visual information.  \n",
        "From the EDA we know:\n",
        "\n",
        "| EDA Finding | Implication for This Notebook |\n",
        "|-------------|------------------------------|\n",
        "| `product_description` is **97.3% missing** | Text data is sparse – image captions become a critical text source |\n",
        "| `bullet_point` is the best text field (89% complete, median 46 words) | Captions will supplement bullet_point, not replace it |\n",
        "| **99.6%** of products (9,190/9,223) have valid image paths | Almost every product can be captioned |\n",
        "| Images are 256px thumbnails in `data/images/small/{prefix}/{hash}.jpg` | Small enough for fast batch inference |\n",
        "\n",
        "### Challenge: Object-of-Interest Extraction\n",
        "\n",
        "Product images may contain background objects (tables, walls, floors) that confuse unconditional captioning.  \n",
        "Example: A curtain product was captioned as \"a wooden table\" because BLIP focused on the table in the background.  \n",
        "**Solution:** Use zero-shot object detection (OWL-ViT) to locate the product before captioning.\n",
        "\n",
        "### Approach: Detect-then-Caption\n",
        "\n",
        "1. **OWL-ViT** (`google/owlvit-base-patch32`) – Zero-shot object detection using product title/category as query to locate and crop the product region.  \n",
        "2. **BLIP** (`Salesforce/blip-image-captioning-base`) – Caption the **cropped** product region (falls back to full image if detection fails).  \n",
        "3. **CLIP** (`openai/clip-vit-base-patch32`) – Validate image-text alignment quality.  \n",
        "4. **Output** – `enriched_products.csv` with `image_caption` and `detection_confidence` columns.\n",
        "\n",
        "### Notebook Outline\n",
        "\n",
        "1. **Step 0** – Environment Setup & Hardware Detection  \n",
        "2. **Step 1** – Load Cleaned Data from EDA  \n",
        "3. **Step 2** – Load BLIP Model (once, GPU-aware)  \n",
        "4. **Step 3** – Batch Image Captioning  \n",
        "5. **Step 4** – Caption Quality Analysis  \n",
        "6. **Step 5** – CLIP Embeddings & Similarity (Optional)  \n",
        "7. **Step 6** – Export Enriched Dataset  \n",
        "8. **Summary & Next Steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 0 – Environment Setup & Hardware Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:08:14.225332Z",
          "iopub.status.busy": "2026-02-10T23:08:14.224583Z",
          "iopub.status.idle": "2026-02-10T23:08:14.234927Z",
          "shell.execute_reply": "2026-02-10T23:08:14.234084Z",
          "shell.execute_reply.started": "2026-02-10T23:08:14.225298Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch 2.8.0+cu126\n",
            "Device: cuda (Tesla T4)\n",
            "\n",
            "Environment ready.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 0: Environment Setup & Hardware Detection\n",
        "# ============================================================\n",
        "# Per constitution: Code must detect mps (Mac) vs cuda (Kaggle/AWS) dynamically.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import gzip\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    OwlViTProcessor,\n",
        "    OwlViTForObjectDetection,\n",
        ")\n",
        "\n",
        "from IPython.display import display, Image as IPImage, HTML\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "\n",
        "# --- Hardware Detection ---\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    GPU_NAME = torch.cuda.get_device_name(0)\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "    GPU_NAME = \"Apple Silicon (MPS)\"\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    GPU_NAME = \"CPU (no GPU)\"\n",
        "\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"PyTorch {torch.__version__}\")\n",
        "print(f\"Device: {DEVICE} ({GPU_NAME})\")\n",
        "print(f\"\\nEnvironment ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:18:48.963057Z",
          "iopub.status.busy": "2026-02-11T00:18:48.962753Z",
          "iopub.status.idle": "2026-02-11T00:18:48.969567Z",
          "shell.execute_reply": "2026-02-11T00:18:48.968876Z",
          "shell.execute_reply.started": "2026-02-11T00:18:48.963033Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMAGE_BASE:    data/images/small\n",
            "OUTPUT_DIR:    /kaggle/working\n",
            "BLIP Model:    Salesforce/blip-image-captioning-base\n",
            "CLIP Model:    openai/clip-vit-base-patch32\n",
            "Batch Size:    32\n",
            "Sample Size:   ALL\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Configuration\n",
        "# ============================================================\n",
        "\n",
        "# --- Paths ---\n",
        "# On Kaggle: data is in /kaggle/working/ or ./data/\n",
        "# Locally: data is in ./data/ relative to notebooks/\n",
        "DATA_DIR = Path(\"./data\")\n",
        "IMAGE_BASE = DATA_DIR / \"images\" / \"small\"\n",
        "\n",
        "# EDA output (from 01-shoptalk-eda.ipynb)\n",
        "# Try Kaggle path first, then local\n",
        "EDA_PKL_PATHS = [\n",
        "    Path(\"/kaggle/working/shoptalk_abo_cleaned.pkl\"),\n",
        "    Path(\"./shoptalk_abo_cleaned.pkl\"),\n",
        "    DATA_DIR / \"shoptalk_abo_cleaned.pkl\",\n",
        "]\n",
        "\n",
        "# Output – always save to /kaggle/working/ so the next notebook can find them\n",
        "OUTPUT_DIR = \"/kaggle/working/\"\n",
        "\n",
        "# --- Model Config ---\n",
        "BLIP_MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
        "CLIP_MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
        "OWLVIT_MODEL_ID = \"google/owlvit-base-patch32\"\n",
        "\n",
        "# --- Processing Config ---\n",
        "BATCH_SIZE = 32 if DEVICE.type == \"cuda\" else 4\n",
        "MAX_CAPTION_TOKENS = 64\n",
        "SAMPLE_SIZE = None  # None = process all; set to e.g. 100 for quick test\n",
        "\n",
        "# --- Detection Config ---\n",
        "DETECTION_CONFIDENCE_THRESHOLD = 0.1  # Minimum OWL-ViT confidence to crop\n",
        "CROP_PADDING_RATIO = 0.05  # Pad the bounding box by 5% on each side\n",
        "\n",
        "print(f\"IMAGE_BASE:    {IMAGE_BASE}\")\n",
        "print(f\"OUTPUT_DIR:    {OUTPUT_DIR}\")\n",
        "print(f\"BLIP Model:    {BLIP_MODEL_ID}\")\n",
        "print(f\"CLIP Model:    {CLIP_MODEL_ID}\")\n",
        "print(f\"Batch Size:    {BATCH_SIZE}\")\n",
        "print(f\"Sample Size:   {SAMPLE_SIZE or 'ALL'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1 – Load Cleaned Data from EDA\n",
        "\n",
        "We load the cleaned DataFrame produced by `01-shoptalk-eda.ipynb`.  \n",
        "Key columns we need: `item_id`, `item_name_flat`, `main_image_id`, `path`, `bullet_point_flat`, `product_type_flat`, `brand_flat`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:27:06.944059Z",
          "iopub.status.busy": "2026-02-10T23:27:06.943434Z",
          "iopub.status.idle": "2026-02-10T23:28:39.757026Z",
          "shell.execute_reply": "2026-02-10T23:28:39.756220Z",
          "shell.execute_reply.started": "2026-02-10T23:27:06.944034Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Success! Loaded 9,223 rows from /kaggle/working/shoptalk_abo_cleaned.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>brand</th>\n",
              "      <th>bullet_point</th>\n",
              "      <th>color</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_name</th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_number</th>\n",
              "      <th>model_year</th>\n",
              "      <th>product_type</th>\n",
              "      <th>style</th>\n",
              "      <th>...</th>\n",
              "      <th>item_keywords_flat</th>\n",
              "      <th>product_description_flat</th>\n",
              "      <th>product_type_flat</th>\n",
              "      <th>item_weight_flat</th>\n",
              "      <th>item_length_flat</th>\n",
              "      <th>title_length</th>\n",
              "      <th>desc_word_count</th>\n",
              "      <th>keyword_count</th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': 'find.'}]</td>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': 'Schoen in Loafer-stijl'}, {'language_tag': 'nl_NL', 'value': 'Platform hak'}, {...</td>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': 'Veelkleurig Vrouw Blauw'}]</td>\n",
              "      <td>B06X9STHNG</td>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': 'Amazon-merk - vinden. Dames Leder Gesloten Teen Hakken,Veelkleurig Vrouw Blauw,...</td>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': '37753'}]</td>\n",
              "      <td>[{'value': '12-05-04'}]</td>\n",
              "      <td>[{'value': 2017}]</td>\n",
              "      <td>[{'value': 'SHOES'}]</td>\n",
              "      <td>[{'language_tag': 'nl_NL', 'value': 'Gesloten-teen pompen'}]</td>\n",
              "      <td>...</td>\n",
              "      <td>block heel shoes | loafer shoes | loafers | metallic shoes | womens block heel shoes | womens fashion | womens loafe...</td>\n",
              "      <td>None</td>\n",
              "      <td>SHOES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>81iZlv3bjpL</td>\n",
              "      <td>8c/8ccb5859.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'language_tag': 'es_MX', 'value': 'AmazonBasics'}]</td>\n",
              "      <td>[{'language_tag': 'es_MX', 'value': 'White Powder Coat Finish'}, {'language_tag': 'es_MX', 'value': '55-Lbs max weig...</td>\n",
              "      <td>[{'language_tag': 'es_MX', 'value': 'White Powder Coat'}]</td>\n",
              "      <td>B07P8ML82R</td>\n",
              "      <td>[{'language_tag': 'es_MX', 'value': '22\" Bottom Mount Drawer Slides, White Powder Coat, 10 Pairs'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': 'AB5013-R22-10'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': 'HARDWARE'}]</td>\n",
              "      <td>[{'language_tag': 'es_MX', 'value': '10 pares'}]</td>\n",
              "      <td>...</td>\n",
              "      <td>22 | Pistola | Montaje bajo | diapositivas | Extensión | Oculto | Deslizadores | Almacenamiento | Bola | Guías | cie...</td>\n",
              "      <td>None</td>\n",
              "      <td>HARDWARE</td>\n",
              "      <td>1.45000</td>\n",
              "      <td>22.00</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>619y9YG9cnL</td>\n",
              "      <td>9f/9f76d27b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'language_tag': 'en_AE', 'value': 'AmazonBasics'}]</td>\n",
              "      <td>[{'language_tag': 'en_AE', 'value': '3D printer filament with 1.75mm diameter + / - .05mm; designed to fit most comm...</td>\n",
              "      <td>[{'language_tag': 'en_AE', 'value': 'Translucent Yellow'}]</td>\n",
              "      <td>B07H9GMYXS</td>\n",
              "      <td>[{'language_tag': 'en_AE', 'value': 'AmazonBasics PETG 3D Printer Filament, 1.75mm, 1 kg Spool 1.75mm AMG10528516-10'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': 'AMG10528516-10'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': 'MECHANICAL_COMPONENTS'}]</td>\n",
              "      <td>[{'language_tag': 'en_AE', 'value': '1-Pack'}]</td>\n",
              "      <td>...</td>\n",
              "      <td>3d printer filament | petg printer filament | petg filament | 1.75mm printer filament | 1kg spool printer filament |...</td>\n",
              "      <td>None</td>\n",
              "      <td>MECHANICAL_COMPONENTS</td>\n",
              "      <td>2.20000</td>\n",
              "      <td>9.21</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>81NP7qh2L6L</td>\n",
              "      <td>66/665cc994.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'language_tag': 'en_GB', 'value': 'Stone &amp; Beam'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'language_tag': 'en_GB', 'value': 'Stone Brown'}]</td>\n",
              "      <td>B07CTPR73M</td>\n",
              "      <td>[{'language_tag': 'en_GB', 'value': 'Stone &amp; Beam Stone Brown Swatch, 25020039-01'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': '25020039-01'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': 'SOFA'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>love | loveseat | queen | for | couch | chesterfield | rolled | couches | button | homelegance | red | daybed | and ...</td>\n",
              "      <td>None</td>\n",
              "      <td>SOFA</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "      <td>61Rp4qOih9L</td>\n",
              "      <td>b4/b4f9d0cc.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'language_tag': 'en_AU', 'value': 'The Fix'}]</td>\n",
              "      <td>[{'language_tag': 'en_AU', 'value': 'Embroidered flowers bloom against understated tan suede in this backless loafer...</td>\n",
              "      <td>[{'language_tag': 'en_AU', 'standardized_values': ['Brown'], 'value': 'Havana Tan'}]</td>\n",
              "      <td>B01MTEI8M6</td>\n",
              "      <td>[{'language_tag': 'en_AU', 'value': 'The Fix Amazon Brand Women's French Floral Embroidery Slide Slip-on Loafer, Hav...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'value': '880071Havana Tan'}]</td>\n",
              "      <td>[{'value': 2017}]</td>\n",
              "      <td>[{'value': 'SHOES'}]</td>\n",
              "      <td>[{'language_tag': 'en_AU', 'value': 'French Loafer Slide'}]</td>\n",
              "      <td>...</td>\n",
              "      <td>zapatos shoe para de ladies mujer womans mocasines | designer fashion moda cocktail work wear business office sexy s...</td>\n",
              "      <td>None</td>\n",
              "      <td>SHOES</td>\n",
              "      <td>0.41875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>714CmIfKIYL</td>\n",
              "      <td>2b/2b1c2516.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  brand  \\\n",
              "0         [{'language_tag': 'nl_NL', 'value': 'find.'}]   \n",
              "1  [{'language_tag': 'es_MX', 'value': 'AmazonBasics'}]   \n",
              "2  [{'language_tag': 'en_AE', 'value': 'AmazonBasics'}]   \n",
              "3  [{'language_tag': 'en_GB', 'value': 'Stone & Beam'}]   \n",
              "4       [{'language_tag': 'en_AU', 'value': 'The Fix'}]   \n",
              "\n",
              "                                                                                                              bullet_point  \\\n",
              "0  [{'language_tag': 'nl_NL', 'value': 'Schoen in Loafer-stijl'}, {'language_tag': 'nl_NL', 'value': 'Platform hak'}, {...   \n",
              "1  [{'language_tag': 'es_MX', 'value': 'White Powder Coat Finish'}, {'language_tag': 'es_MX', 'value': '55-Lbs max weig...   \n",
              "2  [{'language_tag': 'en_AE', 'value': '3D printer filament with 1.75mm diameter + / - .05mm; designed to fit most comm...   \n",
              "3                                                                                                                      NaN   \n",
              "4  [{'language_tag': 'en_AU', 'value': 'Embroidered flowers bloom against understated tan suede in this backless loafer...   \n",
              "\n",
              "                                                                                  color  \\\n",
              "0                       [{'language_tag': 'nl_NL', 'value': 'Veelkleurig Vrouw Blauw'}]   \n",
              "1                             [{'language_tag': 'es_MX', 'value': 'White Powder Coat'}]   \n",
              "2                            [{'language_tag': 'en_AE', 'value': 'Translucent Yellow'}]   \n",
              "3                                   [{'language_tag': 'en_GB', 'value': 'Stone Brown'}]   \n",
              "4  [{'language_tag': 'en_AU', 'standardized_values': ['Brown'], 'value': 'Havana Tan'}]   \n",
              "\n",
              "      item_id  \\\n",
              "0  B06X9STHNG   \n",
              "1  B07P8ML82R   \n",
              "2  B07H9GMYXS   \n",
              "3  B07CTPR73M   \n",
              "4  B01MTEI8M6   \n",
              "\n",
              "                                                                                                                 item_name  \\\n",
              "0  [{'language_tag': 'nl_NL', 'value': 'Amazon-merk - vinden. Dames Leder Gesloten Teen Hakken,Veelkleurig Vrouw Blauw,...   \n",
              "1                      [{'language_tag': 'es_MX', 'value': '22\" Bottom Mount Drawer Slides, White Powder Coat, 10 Pairs'}]   \n",
              "2  [{'language_tag': 'en_AE', 'value': 'AmazonBasics PETG 3D Printer Filament, 1.75mm, 1 kg Spool 1.75mm AMG10528516-10'}]   \n",
              "3                                     [{'language_tag': 'en_GB', 'value': 'Stone & Beam Stone Brown Swatch, 25020039-01'}]   \n",
              "4  [{'language_tag': 'en_AU', 'value': 'The Fix Amazon Brand Women's French Floral Embroidery Slide Slip-on Loafer, Hav...   \n",
              "\n",
              "                                      model_name  \\\n",
              "0  [{'language_tag': 'nl_NL', 'value': '37753'}]   \n",
              "1                                            NaN   \n",
              "2                                            NaN   \n",
              "3                                            NaN   \n",
              "4                                            NaN   \n",
              "\n",
              "                      model_number         model_year  \\\n",
              "0          [{'value': '12-05-04'}]  [{'value': 2017}]   \n",
              "1     [{'value': 'AB5013-R22-10'}]                NaN   \n",
              "2    [{'value': 'AMG10528516-10'}]                NaN   \n",
              "3       [{'value': '25020039-01'}]                NaN   \n",
              "4  [{'value': '880071Havana Tan'}]  [{'value': 2017}]   \n",
              "\n",
              "                           product_type  \\\n",
              "0                  [{'value': 'SHOES'}]   \n",
              "1               [{'value': 'HARDWARE'}]   \n",
              "2  [{'value': 'MECHANICAL_COMPONENTS'}]   \n",
              "3                   [{'value': 'SOFA'}]   \n",
              "4                  [{'value': 'SHOES'}]   \n",
              "\n",
              "                                                          style  ...  \\\n",
              "0  [{'language_tag': 'nl_NL', 'value': 'Gesloten-teen pompen'}]  ...   \n",
              "1              [{'language_tag': 'es_MX', 'value': '10 pares'}]  ...   \n",
              "2                [{'language_tag': 'en_AE', 'value': '1-Pack'}]  ...   \n",
              "3                                                           NaN  ...   \n",
              "4   [{'language_tag': 'en_AU', 'value': 'French Loafer Slide'}]  ...   \n",
              "\n",
              "                                                                                                        item_keywords_flat  \\\n",
              "0  block heel shoes | loafer shoes | loafers | metallic shoes | womens block heel shoes | womens fashion | womens loafe...   \n",
              "1  22 | Pistola | Montaje bajo | diapositivas | Extensión | Oculto | Deslizadores | Almacenamiento | Bola | Guías | cie...   \n",
              "2  3d printer filament | petg printer filament | petg filament | 1.75mm printer filament | 1kg spool printer filament |...   \n",
              "3  love | loveseat | queen | for | couch | chesterfield | rolled | couches | button | homelegance | red | daybed | and ...   \n",
              "4  zapatos shoe para de ladies mujer womans mocasines | designer fashion moda cocktail work wear business office sexy s...   \n",
              "\n",
              "  product_description_flat      product_type_flat item_weight_flat  \\\n",
              "0                     None                  SHOES              NaN   \n",
              "1                     None               HARDWARE          1.45000   \n",
              "2                     None  MECHANICAL_COMPONENTS          2.20000   \n",
              "3                     None                   SOFA          0.10000   \n",
              "4                     None                  SHOES          0.41875   \n",
              "\n",
              "  item_length_flat title_length desc_word_count keyword_count     image_id  \\\n",
              "0              NaN           83               0            10  81iZlv3bjpL   \n",
              "1            22.00           59               0            43  619y9YG9cnL   \n",
              "2             9.21           79               0            10  81NP7qh2L6L   \n",
              "3              NaN           44               0            91  61Rp4qOih9L   \n",
              "4              NaN           96               0             2  714CmIfKIYL   \n",
              "\n",
              "              path  \n",
              "0  8c/8ccb5859.jpg  \n",
              "1  9f/9f76d27b.jpg  \n",
              "2  66/665cc994.jpg  \n",
              "3  b4/b4f9d0cc.jpg  \n",
              "4  2b/2b1c2516.jpg  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Unzip silently to the working directory\n",
        "!unzip -o -q ../input/notebooks/balajigurusala/01-shoptalk-eda/_output_.zip -d /kaggle/working/\n",
        "\n",
        "# 2. Define the path where the file now sits\n",
        "pkl_path = Path(\"/kaggle/working/shoptalk_abo_cleaned.pkl\")\n",
        "\n",
        "# 3. Load the data\n",
        "if pkl_path.exists():\n",
        "    df = pd.read_pickle(pkl_path)\n",
        "    print(f\"✓ Success! Loaded {len(df):,} rows from {pkl_path}\")\n",
        "else:\n",
        "    print(\"⚠️ File still not found in /kaggle/working/\")\n",
        "\n",
        "# Preview to ensure it's correct\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:42:18.746439Z",
          "iopub.status.busy": "2026-02-10T23:42:18.745912Z",
          "iopub.status.idle": "2026-02-10T23:42:19.842662Z",
          "shell.execute_reply": "2026-02-10T23:42:19.841967Z",
          "shell.execute_reply.started": "2026-02-10T23:42:18.746409Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loading cleaned data from: /kaggle/working/shoptalk_abo_cleaned.pkl\n",
            "\n",
            "Dataset shape: 9,223 rows × 47 columns\n",
            "\n",
            "Key columns:\n",
            "  item_id                        ✓  (9,223 non-null)\n",
            "  item_name_flat                 ✓  (9,223 non-null)\n",
            "  main_image_id                  ✓  (9,190 non-null)\n",
            "  path                           ✓  (9,190 non-null)\n",
            "  bullet_point_flat              ✓  (8,199 non-null)\n",
            "  product_type_flat              ✓  (9,223 non-null)\n",
            "  brand_flat                     ✓  (9,220 non-null)\n",
            "\n",
            "Products with image paths: 9,190 / 9,223\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 1: Load Cleaned Data from EDA\n",
        "# ============================================================\n",
        "\n",
        "# Update your path list to this:\n",
        "df = None\n",
        "for pkl_path in EDA_PKL_PATHS:\n",
        "    if pkl_path.exists():\n",
        "        print(f\"\\u2713 Loading cleaned data from: {pkl_path}\")\n",
        "        df = pd.read_pickle(pkl_path)\n",
        "        break\n",
        "\n",
        "if df is None:\n",
        "    print(\"\\u26a0\\ufe0f EDA pickle not found. Attempting to load from CSV fallback...\")\n",
        "    csv_fallbacks = [\n",
        "        Path(\"../input/01-shoptalk-eda/shoptalk_abo_cleaned.csv\"),  # New Input Path\n",
        "        Path(\"/kaggle/working/shoptalk_abo_cleaned.csv\"),\n",
        "        Path(\"./shoptalk_abo_cleaned.csv\")\n",
        "    ]\n",
        "    for csv_path in csv_fallbacks:\n",
        "        if csv_path.exists():\n",
        "            print(f\"\\u2713 Loading from CSV: {csv_path}\")\n",
        "            df = pd.read_csv(csv_path)\n",
        "            break\n",
        "\n",
        "assert df is not None, (\n",
        "    \"Could not find EDA output. Run 01-shoptalk-eda.ipynb first, \"\n",
        "    \"or place shoptalk_abo_cleaned.pkl in one of the expected paths.\"\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape[0]:,} rows \\u00d7 {df.shape[1]} columns\")\n",
        "print(f\"\\nKey columns:\")\n",
        "for col in [\"item_id\", \"item_name_flat\", \"main_image_id\", \"path\",\n",
        "            \"bullet_point_flat\", \"product_type_flat\", \"brand_flat\"]:\n",
        "    present = col in df.columns\n",
        "    non_null = df[col].notna().sum() if present else 0\n",
        "    print(f\"  {col:30s} {'\\u2713' if present else '\\u2717'}  ({non_null:,} non-null)\")\n",
        "\n",
        "# Filter to products with valid image paths\n",
        "df_with_images = df[df[\"path\"].notna()].copy()\n",
        "print(f\"\\nProducts with image paths: {len(df_with_images):,} / {len(df):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:59:14.680237Z",
          "iopub.status.busy": "2026-02-10T23:59:14.679501Z",
          "iopub.status.idle": "2026-02-10T23:59:14.689038Z",
          "shell.execute_reply": "2026-02-10T23:59:14.688330Z",
          "shell.execute_reply.started": "2026-02-10T23:59:14.680211Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image path verification (first 50): 50/50 exist on disk\n",
            "✓ Image paths verified. IMAGE_BASE = data/images/small\n"
          ]
        }
      ],
      "source": [
        "# --- Verify image files exist on disk ---\n",
        "def verify_image_path(rel_path: str, base: Path) -> bool:\n",
        "    \"\"\"Check if a relative image path resolves to an existing file.\n",
        "\n",
        "    Args:\n",
        "        rel_path: Relative path from images.csv.gz (e.g. '8c/8ccb5859.jpg').\n",
        "        base: Base directory for images.\n",
        "\n",
        "    Returns:\n",
        "        True if file exists.\n",
        "    \"\"\"\n",
        "    return (base / str(rel_path)).exists()\n",
        "\n",
        "\n",
        "# Quick spot-check: verify first 50 paths\n",
        "sample_paths = df_with_images[\"path\"].head(50)\n",
        "valid_count = sum(verify_image_path(p, IMAGE_BASE) for p in sample_paths)\n",
        "print(f\"Image path verification (first 50): {valid_count}/50 exist on disk\")\n",
        "\n",
        "if valid_count == 0:\n",
        "    print(\"\\n\\u26a0\\ufe0f No images found! Check IMAGE_BASE path.\")\n",
        "    print(f\"   IMAGE_BASE = {IMAGE_BASE}\")\n",
        "    print(f\"   Sample path from DF: {sample_paths.iloc[0]}\")\n",
        "    print(f\"   Full path would be: {IMAGE_BASE / sample_paths.iloc[0]}\")\n",
        "    # Try to auto-detect\n",
        "    for candidate in [DATA_DIR / \"images\" / \"small\", Path(\"data/images/small\"),\n",
        "                      Path(\"/kaggle/working/data/images/small\")]:\n",
        "        if candidate.exists():\n",
        "            test_path = candidate / str(sample_paths.iloc[0])\n",
        "            if test_path.exists():\n",
        "                IMAGE_BASE = candidate\n",
        "                print(f\"   \\u2713 Auto-detected correct IMAGE_BASE: {IMAGE_BASE}\")\n",
        "                break\n",
        "else:\n",
        "    print(f\"\\u2713 Image paths verified. IMAGE_BASE = {IMAGE_BASE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:44:47.154478Z",
          "iopub.status.busy": "2026-02-10T23:44:47.153766Z",
          "iopub.status.idle": "2026-02-10T23:44:47.309819Z",
          "shell.execute_reply": "2026-02-10T23:44:47.309036Z",
          "shell.execute_reply.started": "2026-02-10T23:44:47.154447Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00  0d\t1a  27\t34  41\t4e  5b\t68  75\t82  8f\t9c  a9\tb6  c3\td0  dd\tea  f7\n",
            "01  0e\t1b  28\t35  42\t4f  5c\t69  76\t83  90\t9d  aa\tb7  c4\td1  de\teb  f8\n",
            "02  0f\t1c  29\t36  43\t50  5d\t6a  77\t84  91\t9e  ab\tb8  c5\td2  df\tec  f9\n",
            "03  10\t1d  2a\t37  44\t51  5e\t6b  78\t85  92\t9f  ac\tb9  c6\td3  e0\ted  fa\n",
            "04  11\t1e  2b\t38  45\t52  5f\t6c  79\t86  93\ta0  ad\tba  c7\td4  e1\tee  fb\n",
            "05  12\t1f  2c\t39  46\t53  60\t6d  7a\t87  94\ta1  ae\tbb  c8\td5  e2\tef  fc\n",
            "06  13\t20  2d\t3a  47\t54  61\t6e  7b\t88  95\ta2  af\tbc  c9\td6  e3\tf0  fd\n",
            "07  14\t21  2e\t3b  48\t55  62\t6f  7c\t89  96\ta3  b0\tbd  ca\td7  e4\tf1  fe\n",
            "08  15\t22  2f\t3c  49\t56  63\t70  7d\t8a  97\ta4  b1\tbe  cb\td8  e5\tf2  ff\n",
            "09  16\t23  30\t3d  4a\t57  64\t71  7e\t8b  98\ta5  b2\tbf  cc\td9  e6\tf3\n",
            "0a  17\t24  31\t3e  4b\t58  65\t72  7f\t8c  99\ta6  b3\tc0  cd\tda  e7\tf4\n",
            "0b  18\t25  32\t3f  4c\t59  66\t73  80\t8d  9a\ta7  b4\tc1  ce\tdb  e8\tf5\n",
            "0c  19\t26  33\t40  4d\t5a  67\t74  81\t8e  9b\ta8  b5\tc2  cf\tdc  e9\tf6\n"
          ]
        }
      ],
      "source": [
        "!ls data/images/small "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-10T23:45:14.292618Z",
          "iopub.status.busy": "2026-02-10T23:45:14.292000Z",
          "iopub.status.idle": "2026-02-10T23:45:14.451900Z",
          "shell.execute_reply": "2026-02-10T23:45:14.450974Z",
          "shell.execute_reply.started": "2026-02-10T23:45:14.292587Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00000529.jpg\n",
            "00003a93.jpg\n",
            "000088e1.jpg\n",
            "0000b301.jpg\n",
            "0000b9b8.jpg\n"
          ]
        }
      ],
      "source": [
        "! ls data/images/small/00 | head -5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2 – Load BLIP Model\n",
        "\n",
        "Per constitution: **Models must load once** (not during inference).  \n",
        "We use `Salesforce/blip-image-captioning-base` – a lightweight but effective image captioning model.\n",
        "\n",
        "| Model | Parameters | Speed (T4) | Quality |\n",
        "|-------|-----------|------------|--------|\n",
        "| BLIP-base | 224M | ~15 img/s | Good for product thumbnails |\n",
        "| BLIP-large | 446M | ~8 img/s | Better detail, slower |\n",
        "| BLIP-2 (OPT-2.7b) | 3.7B | ~2 img/s | Best quality, needs more VRAM |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:15:53.887447Z",
          "iopub.status.busy": "2026-02-11T00:15:53.886711Z",
          "iopub.status.idle": "2026-02-11T00:15:56.987530Z",
          "shell.execute_reply": "2026-02-11T00:15:56.986677Z",
          "shell.execute_reply.started": "2026-02-11T00:15:53.887406Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BLIP model: Salesforce/blip-image-captioning-base\n",
            "Target device: cuda\n",
            "Loading BLIP models on both T4 GPUs...\n",
            "Loading Model 0 on cuda:0...\n",
            "Loading Model 1 on cuda:1...\n",
            "✓ Dual-GPU setup ready! (2x throughput)\n",
            "✓ BLIP model loaded in 3.1s\n",
            "  Model dtype: torch.float16\n",
            "  Parameters: 224M\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 2: Load BLIP Model (once, per constitution)\n",
        "# ============================================================\n",
        "\n",
        "print(f\"Loading BLIP model: {BLIP_MODEL_ID}\")\n",
        "print(f\"Target device: {DEVICE}\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# --- Step 2: Load Dual Models for Data Parallelism ---\n",
        "print(f\"Loading BLIP models on both T4 GPUs...\")\n",
        "\n",
        "# Load the processor (shared)\n",
        "blip_processor = BlipProcessor.from_pretrained(BLIP_MODEL_ID)\n",
        "\n",
        "# Load Model Copy 1 on GPU 0\n",
        "print(\"Loading Model 0 on cuda:0...\")\n",
        "model_0 = BlipForConditionalGeneration.from_pretrained(\n",
        "    BLIP_MODEL_ID, \n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda:0\")\n",
        "model_0.eval()\n",
        "\n",
        "# Load Model Copy 2 on GPU 1\n",
        "print(\"Loading Model 1 on cuda:1...\")\n",
        "model_1 = BlipForConditionalGeneration.from_pretrained(\n",
        "    BLIP_MODEL_ID, \n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda:1\")\n",
        "model_1.eval()\n",
        "\n",
        "print(f\"✓ Dual-GPU setup ready! (2x throughput)\")\n",
        "\n",
        "load_time = time.time() - t0\n",
        "print(f\"\\u2713 BLIP model loaded in {load_time:.1f}s\")\n",
        "print(f\"  Model dtype: {next(blip_model.parameters()).dtype}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in blip_model.parameters()) / 1e6:.0f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2.5 – Load OWL-ViT Object Detector\n",
        "\n",
        "**Problem:** BLIP unconditional captioning describes the most visually prominent object, which may be background clutter (tables, walls) instead of the actual product.\n",
        "\n",
        "**Solution:** Use **OWL-ViT** (`google/owlvit-base-patch32`) – a zero-shot open-vocabulary object detector – to locate the product in the image using the title/category as a text query, crop to the bounding box, then caption the cropped region with BLIP.\n",
        "\n",
        "```\n",
        "Image + Title → OWL-ViT → Bounding Box → Crop → BLIP → Focused Caption\n",
        "                                ↓ (no detection)\n",
        "                         Full Image → BLIP → Fallback Caption\n",
        "```\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Model | `google/owlvit-base-patch32` |\n",
        "| Parameters | ~150M |\n",
        "| Input | Image + text queries (product title, category) |\n",
        "| Output | Bounding boxes with confidence scores |\n",
        "| Threshold | 0.1 (configurable via `DETECTION_CONFIDENCE_THRESHOLD`) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 2.5: Load OWL-ViT Object Detector (once, per constitution)\n",
        "# ============================================================\n",
        "\n",
        "print(f\"Loading OWL-ViT model: {OWLVIT_MODEL_ID}\")\n",
        "print(f\"Target device: {DEVICE}\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "owlvit_processor = OwlViTProcessor.from_pretrained(OWLVIT_MODEL_ID)\n",
        "owlvit_model = OwlViTForObjectDetection.from_pretrained(OWLVIT_MODEL_ID)\n",
        "owlvit_model = owlvit_model.to(DEVICE)\n",
        "owlvit_model.eval()\n",
        "\n",
        "owlvit_load_time = time.time() - t0\n",
        "\n",
        "print(f\"✓ OWL-ViT loaded in {owlvit_load_time:.1f}s\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in owlvit_model.parameters()) / 1e6:.0f}M\")\n",
        "print(f\"  Device: {next(owlvit_model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3 – Detect-then-Caption Pipeline\n",
        "\n",
        "We process all products using the **Detect-then-Caption** approach:\n",
        "\n",
        "1. **Build queries** from product title/category for OWL-ViT.  \n",
        "2. **Detect** the product region in the image using OWL-ViT zero-shot detection.  \n",
        "3. **Crop** to the highest-confidence bounding box (with padding).  \n",
        "4. **Caption** the cropped region (or full image on fallback) with BLIP.  \n",
        "5. **Track** detection confidence and hit rate for quality monitoring.\n",
        "\n",
        "Fallback: If OWL-ViT detects nothing above the threshold, BLIP captions the full image (original behavior)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:17:11.902433Z",
          "iopub.status.busy": "2026-02-11T00:17:11.902130Z",
          "iopub.status.idle": "2026-02-11T00:17:11.911014Z",
          "shell.execute_reply": "2026-02-11T00:17:11.910077Z",
          "shell.execute_reply.started": "2026-02-11T00:17:11.902410Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 3: Detect-then-Caption Pipeline\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "def load_image_safe(rel_path: str, base: Path) -> Image.Image:\n",
        "    \"\"\"Load an image from a relative path, returning None on failure.\n",
        "\n",
        "    Args:\n",
        "        rel_path: Relative path to image (e.g. '8c/8ccb5859.jpg').\n",
        "        base: Base directory.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image in RGB, or None if file missing/corrupt.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        full_path = base / str(rel_path)\n",
        "        img = Image.open(full_path).convert(\"RGB\")\n",
        "        return img\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Detection Query Builder ---\n",
        "def build_detection_queries(row) -> list:\n",
        "    \"\"\"Build text queries for OWL-ViT from product metadata.\n",
        "\n",
        "    Extracts meaningful search terms from the product title, category,\n",
        "    and brand to guide zero-shot object detection.\n",
        "\n",
        "    Args:\n",
        "        row: DataFrame row with product metadata.\n",
        "\n",
        "    Returns:\n",
        "        List of 1-3 short query strings for OWL-ViT.\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "\n",
        "    # 1. Category (most reliable – e.g. \"CURTAIN\", \"TABLE_LAMP\")\n",
        "    category = str(row.get(\"product_type_flat\", \"\") or \"\").strip()\n",
        "    if category and category != \"nan\":\n",
        "        # Clean up ABO category format: \"HOME_BED_AND_BATH\" -> \"bed and bath\"\n",
        "        cat_clean = category.lower().replace(\"_\", \" \")\n",
        "        # Take last meaningful segment (often the product type)\n",
        "        segments = cat_clean.split()\n",
        "        if len(segments) > 2:\n",
        "            # Use last 2 words as they tend to be the object type\n",
        "            queries.append(\" \".join(segments[-2:]))\n",
        "        queries.append(cat_clean)\n",
        "\n",
        "    # 2. Title (extract first few meaningful words, skip brand)\n",
        "    title = str(row.get(\"item_name_flat\", \"\") or \"\").strip()\n",
        "    if title and title != \"nan\":\n",
        "        # Remove brand prefix if present\n",
        "        brand = str(row.get(\"brand_flat\", \"\") or \"\").strip()\n",
        "        title_clean = title\n",
        "        if brand and brand != \"nan\" and title_clean.lower().startswith(brand.lower()):\n",
        "            title_clean = title_clean[len(brand):].strip(\" -–—,\")\n",
        "        # Take first 5 meaningful words (skip numbers/symbols)\n",
        "        words = [w for w in title_clean.split() if len(w) > 1 and not w.isdigit()][:5]\n",
        "        if words:\n",
        "            queries.append(\" \".join(words))\n",
        "\n",
        "    # Deduplicate, truncate to CLIP-compatible length\n",
        "    seen = set()\n",
        "    unique_queries = []\n",
        "    for q in queries:\n",
        "        q_lower = q.lower().strip()\n",
        "        if q_lower and q_lower not in seen and len(q_lower) > 2:\n",
        "            seen.add(q_lower)\n",
        "            unique_queries.append(q[:77])  # CLIP max token length\n",
        "    return unique_queries[:3] if unique_queries else [\"product\"]\n",
        "\n",
        "\n",
        "# --- OWL-ViT Object Detection + Crop ---\n",
        "def detect_and_crop(\n",
        "    image: Image.Image,\n",
        "    query_texts: list,\n",
        "    processor,\n",
        "    model,\n",
        "    device,\n",
        "    confidence_threshold: float = DETECTION_CONFIDENCE_THRESHOLD,\n",
        "    padding_ratio: float = CROP_PADDING_RATIO,\n",
        ") -> tuple:\n",
        "    \"\"\"Detect the product of interest and crop to its bounding box.\n",
        "\n",
        "    Uses OWL-ViT zero-shot detection with text queries derived from\n",
        "    the product title/category to locate the product region.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image (full product photo).\n",
        "        query_texts: List of text queries (e.g. [\"curtain\", \"window treatment\"]).\n",
        "        processor: OwlViTProcessor instance.\n",
        "        model: OwlViTForObjectDetection instance.\n",
        "        device: torch device.\n",
        "        confidence_threshold: Minimum confidence to accept a detection.\n",
        "        padding_ratio: Fraction of box size to pad on each side.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cropped_image, confidence, detected).\n",
        "        If no detection above threshold, returns (original_image, None, False).\n",
        "    \"\"\"\n",
        "    w, h = image.size\n",
        "\n",
        "    try:\n",
        "        # OWL-ViT expects queries as a list of lists (one list per image)\n",
        "        inputs = processor(text=[query_texts], images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Post-process: get boxes and scores\n",
        "        target_sizes = torch.tensor([[h, w]], device=device)\n",
        "        results = processor.post_process_object_detection(\n",
        "            outputs, threshold=confidence_threshold, target_sizes=target_sizes\n",
        "        )[0]\n",
        "\n",
        "        scores = results[\"scores\"]\n",
        "        boxes = results[\"boxes\"]\n",
        "\n",
        "        if len(scores) == 0:\n",
        "            return image, None, False\n",
        "\n",
        "        # Pick the highest-confidence detection\n",
        "        best_idx = scores.argmax().item()\n",
        "        best_score = scores[best_idx].item()\n",
        "        best_box = boxes[best_idx].cpu().tolist()  # [x1, y1, x2, y2]\n",
        "\n",
        "        # Add padding\n",
        "        x1, y1, x2, y2 = best_box\n",
        "        box_w = x2 - x1\n",
        "        box_h = y2 - y1\n",
        "        pad_x = box_w * padding_ratio\n",
        "        pad_y = box_h * padding_ratio\n",
        "\n",
        "        x1 = max(0, x1 - pad_x)\n",
        "        y1 = max(0, y1 - pad_y)\n",
        "        x2 = min(w, x2 + pad_x)\n",
        "        y2 = min(h, y2 + pad_y)\n",
        "\n",
        "        # Ensure minimum crop size (at least 32x32 pixels)\n",
        "        if (x2 - x1) < 32 or (y2 - y1) < 32:\n",
        "            return image, best_score, False\n",
        "\n",
        "        cropped = image.crop((int(x1), int(y1), int(x2), int(y2)))\n",
        "        return cropped, best_score, True\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback on any detection error\n",
        "        return image, None, False\n",
        "\n",
        "\n",
        "# --- Single-Batch Caption Function (with detection) ---\n",
        "def caption_batch_on_device(\n",
        "    image_paths, rows, blip_model, blip_proc, owl_proc, owl_model,\n",
        "    base, device, max_tokens=64, conf_threshold=DETECTION_CONFIDENCE_THRESHOLD,\n",
        "):\n",
        "    \"\"\"Caption a batch of images using detect-then-caption pipeline.\n",
        "\n",
        "    For each image:\n",
        "    1. Build detection queries from product metadata.\n",
        "    2. Run OWL-ViT to detect and crop the product region.\n",
        "    3. Caption the cropped (or full) image with BLIP.\n",
        "\n",
        "    Args:\n",
        "        image_paths: List of relative image paths.\n",
        "        rows: List of DataFrame row dicts (for building detection queries).\n",
        "        blip_model: BLIP model instance.\n",
        "        blip_proc: BlipProcessor instance.\n",
        "        owl_proc: OwlViTProcessor instance.\n",
        "        owl_model: OwlViTForObjectDetection instance.\n",
        "        base: Image base directory path.\n",
        "        device: torch device for OWL-ViT inference.\n",
        "        max_tokens: Max BLIP caption tokens.\n",
        "        conf_threshold: OWL-ViT confidence threshold.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (captions_list, confidences_list, detected_flags_list).\n",
        "    \"\"\"\n",
        "    images_for_caption = []\n",
        "    valid_indices = []\n",
        "    confidences = [None] * len(image_paths)\n",
        "    detected_flags = [False] * len(image_paths)\n",
        "\n",
        "    for i, (path, row) in enumerate(zip(image_paths, rows)):\n",
        "        img = load_image_safe(path, base)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Step 1: Build queries from metadata\n",
        "        queries = build_detection_queries(row)\n",
        "\n",
        "        # Step 2: Detect and crop\n",
        "        cropped_img, conf, was_detected = detect_and_crop(\n",
        "            img, queries, owl_proc, owl_model, device, conf_threshold\n",
        "        )\n",
        "\n",
        "        images_for_caption.append(cropped_img)\n",
        "        valid_indices.append(i)\n",
        "        confidences[i] = conf\n",
        "        detected_flags[i] = was_detected\n",
        "\n",
        "    if not images_for_caption:\n",
        "        return [None] * len(image_paths), confidences, detected_flags\n",
        "\n",
        "    # Step 3: Caption the (cropped) images with BLIP\n",
        "    inputs = blip_proc(images=images_for_caption, return_tensors=\"pt\", padding=True).to(blip_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = blip_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "    captions_raw = blip_proc.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    result = [None] * len(image_paths)\n",
        "    for idx, caption in zip(valid_indices, captions_raw):\n",
        "        result[idx] = caption.strip()\n",
        "\n",
        "    return result, confidences, detected_flags\n",
        "\n",
        "\n",
        "# --- Dual-GPU Splitter (updated for detect-then-caption) ---\n",
        "def process_dual_batch(batch_paths, batch_rows):\n",
        "    \"\"\"Split a batch across two GPUs for parallel inference.\n",
        "\n",
        "    OWL-ViT runs on CPU/primary device; BLIP models are on cuda:0 and cuda:1.\n",
        "    \"\"\"\n",
        "    mid = len(batch_paths) // 2\n",
        "    paths_0, paths_1 = batch_paths[:mid], batch_paths[mid:]\n",
        "    rows_0, rows_1 = batch_rows[:mid], batch_rows[mid:]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        future_0 = executor.submit(\n",
        "            caption_batch_on_device, paths_0, rows_0,\n",
        "            model_0, blip_processor, owlvit_processor, owlvit_model,\n",
        "            IMAGE_BASE, DEVICE, MAX_CAPTION_TOKENS,\n",
        "        )\n",
        "        future_1 = executor.submit(\n",
        "            caption_batch_on_device, paths_1, rows_1,\n",
        "            model_1, blip_processor, owlvit_processor, owlvit_model,\n",
        "            IMAGE_BASE, DEVICE, MAX_CAPTION_TOKENS,\n",
        "        )\n",
        "\n",
        "        caps_0, confs_0, dets_0 = future_0.result()\n",
        "        caps_1, confs_1, dets_1 = future_1.result()\n",
        "\n",
        "        return caps_0 + caps_1, confs_0 + confs_1, dets_0 + dets_1\n",
        "\n",
        "\n",
        "# --- Single-GPU Fallback ---\n",
        "def process_single_batch(batch_paths, batch_rows, blip_model):\n",
        "    \"\"\"Process batch on a single device (CPU/MPS/single GPU).\"\"\"\n",
        "    return caption_batch_on_device(\n",
        "        batch_paths, batch_rows,\n",
        "        blip_model, blip_processor, owlvit_processor, owlvit_model,\n",
        "        IMAGE_BASE, DEVICE, MAX_CAPTION_TOKENS,\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"✓ Detect-then-caption functions defined.\")\n",
        "print(f\"  Detection threshold: {DETECTION_CONFIDENCE_THRESHOLD}\")\n",
        "print(f\"  Crop padding ratio:  {CROP_PADDING_RATIO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:19:23.972882Z",
          "iopub.status.busy": "2026-02-11T00:19:23.972307Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory cleared. GPU 0 free: 7.81 GB\n",
            "Restarting with Batch Size: 32 (Split between 2 GPUs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b7bdc7d6fcb4d8f8c9fbbe7f764a657",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dual-GPU Captioning:   0%|          | 0/288 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# 1. Clear memory\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Memory cleared. GPU 0 free: {torch.cuda.mem_get_info(0)[0]/1e9:.2f} GB\")\n",
        "\n",
        "# 2. Reset results\n",
        "all_captions = []\n",
        "all_confidences = []\n",
        "all_detected = []\n",
        "\n",
        "# 3. Prepare row dicts for detection query building\n",
        "all_paths = df_caption[\"path\"].tolist()\n",
        "all_rows = df_caption.to_dict(\"records\")\n",
        "\n",
        "# 4. Determine processing mode\n",
        "use_dual_gpu = (\n",
        "    torch.cuda.is_available()\n",
        "    and torch.cuda.device_count() >= 2\n",
        "    and hasattr(globals().get(\"model_0\", None) or object(), \"device\")\n",
        ")\n",
        "mode_label = \"2x T4 GPUs (Dual)\" if use_dual_gpu else f\"{DEVICE} (Single)\"\n",
        "\n",
        "print(f\"Detect-then-Caption Pipeline\")\n",
        "print(f\"  Batch Size:  {BATCH_SIZE}\")\n",
        "print(f\"  Mode:        {mode_label}\")\n",
        "print(f\"  Images:      {len(all_paths):,}\")\n",
        "print(f\"  Detection:   OWL-ViT (threshold={DETECTION_CONFIDENCE_THRESHOLD})\")\n",
        "\n",
        "t0 = time.time()\n",
        "n_batches = (len(all_paths) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "for batch_start in tqdm(range(0, len(all_paths), BATCH_SIZE), total=n_batches, desc=\"Detect+Caption\"):\n",
        "    batch_paths = all_paths[batch_start : batch_start + BATCH_SIZE]\n",
        "    batch_rows = all_rows[batch_start : batch_start + BATCH_SIZE]\n",
        "\n",
        "    try:\n",
        "        if use_dual_gpu:\n",
        "            caps, confs, dets = process_dual_batch(batch_paths, batch_rows)\n",
        "        else:\n",
        "            # Single-GPU / MPS / CPU fallback\n",
        "            blip_mdl = model_0 if \"model_0\" in dir() else blip_model\n",
        "            caps, confs, dets = process_single_batch(batch_paths, batch_rows, blip_mdl)\n",
        "\n",
        "        all_captions.extend(caps)\n",
        "        all_confidences.extend(confs)\n",
        "        all_detected.extend(dets)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in batch {batch_start}: {e}\")\n",
        "        all_captions.extend([None] * len(batch_paths))\n",
        "        all_confidences.extend([None] * len(batch_paths))\n",
        "        all_detected.extend([False] * len(batch_paths))\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "n_success = sum(1 for c in all_captions if c is not None)\n",
        "n_detected = sum(1 for d in all_detected if d)\n",
        "throughput = n_success / elapsed if elapsed > 0 else 0\n",
        "\n",
        "# Save to DataFrame\n",
        "df_caption[\"image_caption\"] = all_captions\n",
        "df_caption[\"detection_confidence\"] = all_confidences\n",
        "df_caption[\"detection_hit\"] = all_detected\n",
        "\n",
        "print(f\"\\n✅ Detect-then-Caption complete!\")\n",
        "print(f\"   Time:            {elapsed:.1f}s\")\n",
        "print(f\"   Throughput:      {throughput:.1f} images/sec\")\n",
        "print(f\"   Mode:            {mode_label}\")\n",
        "print(f\"   Detection hits:  {n_detected:,} / {n_success:,} ({n_detected/max(n_success,1):.1%})\")\n",
        "print(f\"   Fallback (full): {n_success - n_detected:,} ({(n_success - n_detected)/max(n_success,1):.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:25:56.233830Z",
          "iopub.status.busy": "2026-02-11T00:25:56.233222Z",
          "iopub.status.idle": "2026-02-11T00:25:56.238672Z",
          "shell.execute_reply": "2026-02-11T00:25:56.237891Z",
          "shell.execute_reply.started": "2026-02-11T00:25:56.233805Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Captioning complete!\n",
            "   Total images: 9,190\n",
            "   Successful:   9,190 (100.0%)\n",
            "   Failed:       0\n",
            "   Time:         355.4s\n",
            "   Throughput:   25.9 images/sec\n",
            "   Device used:  cuda (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"DETECT-THEN-CAPTION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Total images:        {len(all_captions):,}\")\n",
        "print(f\"  Successful captions: {n_success:,} ({n_success/len(all_captions):.1%})\")\n",
        "print(f\"  Failed:              {len(all_captions) - n_success:,}\")\n",
        "print(f\"  Detection hits:      {n_detected:,} ({n_detected/max(n_success,1):.1%}) — OWL-ViT found product\")\n",
        "print(f\"  Fallback (full img): {n_success - n_detected:,} ({(n_success - n_detected)/max(n_success,1):.1%})\")\n",
        "print(f\"  Time:                {elapsed:.1f}s\")\n",
        "print(f\"  Throughput:          {throughput:.1f} images/sec\")\n",
        "print(f\"  Device used:         {DEVICE} ({GPU_NAME})\")\n",
        "\n",
        "# Detection confidence distribution\n",
        "valid_confs = [c for c in all_confidences if c is not None]\n",
        "if valid_confs:\n",
        "    confs_arr = np.array(valid_confs)\n",
        "    print(f\"\\n  Detection confidence stats:\")\n",
        "    print(f\"    Mean:   {confs_arr.mean():.3f}\")\n",
        "    print(f\"    Median: {np.median(confs_arr):.3f}\")\n",
        "    print(f\"    Min:    {confs_arr.min():.3f}\")\n",
        "    print(f\"    Max:    {confs_arr.max():.3f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-11T00:26:01.429695Z",
          "iopub.status.busy": "2026-02-11T00:26:01.429123Z",
          "iopub.status.idle": "2026-02-11T00:26:01.476934Z",
          "shell.execute_reply": "2026-02-11T00:26:01.476375Z",
          "shell.execute_reply.started": "2026-02-11T00:26:01.429668Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "│ Title:    Stone & ♛ Marbury 84 Poutre de Traitement pour fenêtre Bleu Marine\n",
            "│ Category: HOME_BED_AND_BATH\n",
            "│ Caption:  a wooden table\n",
            "│ Bullets:  None\n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "│ Title:    Amazon Brand – Stone & Beam Andover Modern Rectangle Ottoman, 32\"W, Grey\n",
            "│ Category: OTTOMAN\n",
            "│ Caption:  a gray ottoman with wooden legs and a small foot stool\n",
            "│ Bullets:  81.28 cm 宽 x 63.50 cm 深 x 53.34 cm 高；座椅深度：63.50 cm 深；座椅靠背高度：53.34 cm 高；腿高度：10.80 cm 高 | 硬木松木框架上的防污聚酯面料 | 流线型设计是现代设计的标志 |\n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "│ Title:    AmazonBasics 7-Piece Bed-in-A-Bag - Full/Queen, Navy Simple Plaid (Renewed)\n",
            "│ Category: HOME_BED_AND_BATH\n",
            "│ Caption:  a bed with a blue comforter and pillows\n",
            "│ Bullets:  Includes 86 x 90 inch comforter, 90 x 102 inch flat sheet, 60 x 80 inch fitted sheet, two 20 x 26 inch shams, and two 20\n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "│ Title:    [Amazonブランド] Umi.(ウミ) 強力粘着フック 壁掛けフック コートフック タオル掛け キッチン 洗面所浴室 ステンレス, A7060H3-2\n",
            "│ Category: HOME\n",
            "│ Caption:  a picture of a pineapple hanging on a hook\n",
            "│ Bullets:  【穴開け不要】：強力な粘着力があり、取り外すのも簡単です。防水設計があるので、お風呂場や洗面所などのところで使用できます。 | 【痕跡残らず】：アンカーを使うことは必要ないため、壁を傷つけません。そして取り外した後、痕跡がありません。 | \n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "│ Title:    Amazon Brand - Solimo Designer Unicorn Texture UV Printed Soft Back Case Mobile \n",
            "│ Category: CELLULAR_PHONE_CASE\n",
            "│ Caption:  unicorns and rainbows iphone case\n",
            "│ Bullets:  Snug fit for Coolpad Note 6, with perfect cut-outs for volume buttons, audio and charging ports | Compatible with Coolpa\n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "# --- Preview: Show 5 sample captions with detection info ---\n",
        "\n",
        "sample = df_caption[df_caption[\"image_caption\"].notna()].sample(\n",
        "    min(5, n_success), random_state=42\n",
        ")\n",
        "\n",
        "for _, row in sample.iterrows():\n",
        "    img_path = IMAGE_BASE / str(row[\"path\"])\n",
        "    title = str(row.get(\"item_name_flat\", \"N/A\"))[:80]\n",
        "    caption = row[\"image_caption\"]\n",
        "    bullet = str(row.get(\"bullet_point_flat\", \"N/A\"))[:120]\n",
        "    category = str(row.get(\"product_type_flat\", \"N/A\"))\n",
        "    det_conf = row.get(\"detection_confidence\")\n",
        "    det_hit = row.get(\"detection_hit\", False)\n",
        "    det_label = f\"✓ detected (conf={det_conf:.2f})\" if det_hit else \"✗ fallback (full image)\"\n",
        "\n",
        "    if img_path.exists():\n",
        "        display(IPImage(filename=str(img_path), width=200))\n",
        "\n",
        "    print(f\"│ Title:     {title}\")\n",
        "    print(f\"│ Category:  {category}\")\n",
        "    print(f\"│ Detection: {det_label}\")\n",
        "    print(f\"│ Caption:   {caption}\")\n",
        "    print(f\"│ Bullets:   {bullet}\")\n",
        "    print(\"─\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4 – Caption Quality & Detection Analysis\n",
        "\n",
        "We analyze the generated captions to assess quality and the impact of detect-then-caption:\n",
        "\n",
        "1. **Detection hit rate** and confidence score distribution.  \n",
        "2. Caption length distribution (words and characters).  \n",
        "3. Most frequent caption words (word cloud).  \n",
        "4. Overlap between caption vocabulary and title/bullet vocabulary.  \n",
        "5. **Side-by-side comparison**: detected (cropped) vs. fallback (full image) captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 4: Caption Quality Analysis\n",
        "# ============================================================\n",
        "\n",
        "captions_valid = df_caption[df_caption[\"image_caption\"].notna()][\"image_caption\"]\n",
        "\n",
        "# --- Length distributions ---\n",
        "caption_word_counts = captions_valid.apply(lambda x: len(x.split()))\n",
        "caption_char_counts = captions_valid.apply(len)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Word count\n",
        "axes[0].hist(caption_word_counts, bins=30, color=\"teal\", edgecolor=\"white\", alpha=0.8)\n",
        "axes[0].axvline(caption_word_counts.median(), color=\"red\", linestyle=\"--\",\n",
        "                label=f\"Median: {caption_word_counts.median():.0f} words\")\n",
        "axes[0].set_xlabel(\"Word Count\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].set_title(\"Caption Word Count Distribution\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Character count\n",
        "axes[1].hist(caption_char_counts, bins=30, color=\"coral\", edgecolor=\"white\", alpha=0.8)\n",
        "axes[1].axvline(caption_char_counts.median(), color=\"darkred\", linestyle=\"--\",\n",
        "                label=f\"Median: {caption_char_counts.median():.0f} chars\")\n",
        "axes[1].set_xlabel(\"Character Count\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].set_title(\"Caption Character Count Distribution\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle(\"BLIP Caption Length Analysis\", fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCaption statistics:\")\n",
        "print(f\"  Mean word count:   {caption_word_counts.mean():.1f}\")\n",
        "print(f\"  Median word count: {caption_word_counts.median():.0f}\")\n",
        "print(f\"  Min / Max:         {caption_word_counts.min()} / {caption_word_counts.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 4a: Detection Analysis – OWL-ViT Performance\n",
        "# ============================================================\n",
        "\n",
        "# --- Detection Hit Rate ---\n",
        "n_total_captioned = df_caption[\"image_caption\"].notna().sum()\n",
        "n_det_hits = df_caption[\"detection_hit\"].sum()\n",
        "n_fallback = n_total_captioned - n_det_hits\n",
        "\n",
        "print(f\"Detection Summary:\")\n",
        "print(f\"  Total captioned:  {n_total_captioned:,}\")\n",
        "print(f\"  OWL-ViT detected: {n_det_hits:,} ({n_det_hits/max(n_total_captioned,1):.1%})\")\n",
        "print(f\"  Fallback (full):  {n_fallback:,} ({n_fallback/max(n_total_captioned,1):.1%})\")\n",
        "\n",
        "# --- Confidence Distribution ---\n",
        "valid_confs = df_caption[\"detection_confidence\"].dropna()\n",
        "\n",
        "if len(valid_confs) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    # Confidence histogram\n",
        "    axes[0].hist(valid_confs, bins=40, color=\"steelblue\", edgecolor=\"white\", alpha=0.85)\n",
        "    axes[0].axvline(valid_confs.median(), color=\"red\", linestyle=\"--\",\n",
        "                    label=f\"Median: {valid_confs.median():.3f}\")\n",
        "    axes[0].axvline(DETECTION_CONFIDENCE_THRESHOLD, color=\"orange\", linestyle=\":\",\n",
        "                    label=f\"Threshold: {DETECTION_CONFIDENCE_THRESHOLD}\")\n",
        "    axes[0].set_xlabel(\"OWL-ViT Confidence Score\")\n",
        "    axes[0].set_ylabel(\"Frequency\")\n",
        "    axes[0].set_title(\"Detection Confidence Distribution\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Detection hit rate by category (top 15)\n",
        "    det_by_cat = df_caption[df_caption[\"image_caption\"].notna()].groupby(\"product_type_flat\").agg(\n",
        "        total=(\"detection_hit\", \"count\"),\n",
        "        hits=(\"detection_hit\", \"sum\"),\n",
        "    )\n",
        "    det_by_cat[\"hit_rate\"] = det_by_cat[\"hits\"] / det_by_cat[\"total\"]\n",
        "    det_by_cat = det_by_cat[det_by_cat[\"total\"] >= 10].sort_values(\"hit_rate\", ascending=True).tail(15)\n",
        "\n",
        "    axes[1].barh(det_by_cat.index, det_by_cat[\"hit_rate\"], color=\"teal\", alpha=0.8)\n",
        "    axes[1].set_xlabel(\"Detection Hit Rate\")\n",
        "    axes[1].set_title(\"OWL-ViT Hit Rate by Category (≥10 products)\")\n",
        "    axes[1].set_xlim(0, 1)\n",
        "\n",
        "    plt.suptitle(\"OWL-ViT Object Detection Performance\", fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"  No valid detection confidences to plot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Word Cloud of Captions ---\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "all_caption_text = \" \".join(captions_valid.tolist())\n",
        "\n",
        "wc = WordCloud(\n",
        "    width=900, height=400,\n",
        "    background_color=\"white\",\n",
        "    max_words=150,\n",
        "    colormap=\"viridis\",\n",
        "    random_state=42,\n",
        ").generate(all_caption_text)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.imshow(wc, interpolation=\"bilinear\")\n",
        "ax.axis(\"off\")\n",
        "ax.set_title(\"Word Cloud \\u2013 BLIP Image Captions\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Top 20 most frequent caption phrases ---\n",
        "from collections import Counter\n",
        "\n",
        "# Count full captions (many will be similar for similar products)\n",
        "caption_counts = Counter(captions_valid.tolist())\n",
        "top_20 = caption_counts.most_common(20)\n",
        "\n",
        "print(\"Top 20 Most Frequent Captions:\")\n",
        "print(\"-\" * 60)\n",
        "for i, (caption, count) in enumerate(top_20, 1):\n",
        "    print(f\"  {i:2d}. [{count:4d}x] {caption[:80]}\")\n",
        "\n",
        "# Unique caption ratio\n",
        "n_unique = len(set(captions_valid.tolist()))\n",
        "print(f\"\\nUnique captions: {n_unique:,} / {len(captions_valid):,} ({n_unique/len(captions_valid):.1%})\")\n",
        "print(f\"Caption diversity score: {n_unique/len(captions_valid):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Caption vs. Existing Text Overlap ---\n",
        "# Do captions add NEW information beyond what title/bullets already provide?\n",
        "\n",
        "\n",
        "def jaccard_similarity(text_a: str, text_b: str) -> float:\n",
        "    \"\"\"Compute Jaccard similarity between two texts (word-level).\n",
        "\n",
        "    Args:\n",
        "        text_a: First text.\n",
        "        text_b: Second text.\n",
        "\n",
        "    Returns:\n",
        "        Jaccard index (0 to 1).\n",
        "    \"\"\"\n",
        "    words_a = set(str(text_a).lower().split())\n",
        "    words_b = set(str(text_b).lower().split())\n",
        "    if not words_a or not words_b:\n",
        "        return 0.0\n",
        "    intersection = words_a & words_b\n",
        "    union = words_a | words_b\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "\n",
        "# Compute overlap for products that have both caption and bullet_point\n",
        "overlap_df = df_caption[\n",
        "    df_caption[\"image_caption\"].notna() & df_caption[\"bullet_point_flat\"].notna()\n",
        "].copy()\n",
        "\n",
        "if len(overlap_df) > 0:\n",
        "    overlap_df[\"caption_vs_bullet_jaccard\"] = overlap_df.apply(\n",
        "        lambda r: jaccard_similarity(r[\"image_caption\"], r[\"bullet_point_flat\"]), axis=1\n",
        "    )\n",
        "    overlap_df[\"caption_vs_title_jaccard\"] = overlap_df.apply(\n",
        "        lambda r: jaccard_similarity(r[\"image_caption\"], r[\"item_name_flat\"]), axis=1\n",
        "    )\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    axes[0].hist(overlap_df[\"caption_vs_bullet_jaccard\"], bins=30, color=\"teal\",\n",
        "                 edgecolor=\"white\", alpha=0.8)\n",
        "    axes[0].set_xlabel(\"Jaccard Similarity\")\n",
        "    axes[0].set_ylabel(\"Frequency\")\n",
        "    axes[0].set_title(\"Caption vs. Bullet Point Overlap\")\n",
        "    axes[0].axvline(overlap_df[\"caption_vs_bullet_jaccard\"].median(), color=\"red\",\n",
        "                    linestyle=\"--\",\n",
        "                    label=f\"Median: {overlap_df['caption_vs_bullet_jaccard'].median():.3f}\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].hist(overlap_df[\"caption_vs_title_jaccard\"], bins=30, color=\"coral\",\n",
        "                 edgecolor=\"white\", alpha=0.8)\n",
        "    axes[1].set_xlabel(\"Jaccard Similarity\")\n",
        "    axes[1].set_ylabel(\"Frequency\")\n",
        "    axes[1].set_title(\"Caption vs. Product Title Overlap\")\n",
        "    axes[1].axvline(overlap_df[\"caption_vs_title_jaccard\"].median(), color=\"darkred\",\n",
        "                    linestyle=\"--\",\n",
        "                    label=f\"Median: {overlap_df['caption_vs_title_jaccard'].median():.3f}\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.suptitle(\"Do Captions Add New Information?\", fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nOverlap analysis ({len(overlap_df):,} products with both caption + bullet):\")\n",
        "    print(f\"  Caption vs Bullet Jaccard:  mean={overlap_df['caption_vs_bullet_jaccard'].mean():.3f}\")\n",
        "    print(f\"  Caption vs Title Jaccard:   mean={overlap_df['caption_vs_title_jaccard'].mean():.3f}\")\n",
        "    print(f\"  \\u2192 Low overlap = captions provide COMPLEMENTARY information \\u2713\")\n",
        "else:\n",
        "    print(\"Insufficient data for overlap analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations \\u2013 Caption Quality**\n",
        "\n",
        "- *Populate after running:*\n",
        "  - **Detection hit rate:** What % of products did OWL-ViT successfully detect?\n",
        "  - **Confidence distribution:** How confident is OWL-ViT on detected products?\n",
        "  - **Hit rate by category:** Which product categories are easiest/hardest to detect?\n",
        "  - Median caption length (words/chars).\n",
        "  - Caption diversity: what % are unique?\n",
        "  - Common caption patterns (e.g., \\\"a close up of a...\\\").\n",
        "  - Jaccard overlap with bullets/title: low = captions add new visual info."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5 – CLIP Embeddings & Similarity (Optional)\n",
        "\n",
        "We use **CLIP** to generate image embeddings and compare them with text embeddings.  \n",
        "This validates that the visual content of images aligns with their captions and titles.\n",
        "\n",
        "CLIP is not used for captioning (that's BLIP's job), but for:\n",
        "- Verifying image-text alignment quality.\n",
        "- Providing an alternative embedding for image-based search (future)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 5: CLIP Embeddings & Image-Text Similarity\n",
        "# ============================================================\n",
        "\n",
        "print(f\"Loading CLIP model: {CLIP_MODEL_ID}\")\n",
        "t0 = time.time()\n",
        "\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE)\n",
        "clip_model.eval()\n",
        "\n",
        "load_time = time.time() - t0\n",
        "print(f\"\\u2713 CLIP model loaded in {load_time:.1f}s\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in clip_model.parameters()) / 1e6:.0f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Compute CLIP similarity: image vs caption, image vs title ---\n",
        "\n",
        "# Sample for speed (CLIP is slower to run on all)\n",
        "clip_sample_size = min(500, len(df_caption[df_caption[\"image_caption\"].notna()]))\n",
        "df_clip = df_caption[df_caption[\"image_caption\"].notna()].sample(\n",
        "    clip_sample_size, random_state=42\n",
        ").copy()\n",
        "\n",
        "print(f\"Computing CLIP similarities for {clip_sample_size} products...\")\n",
        "\n",
        "sim_caption = []\n",
        "sim_title = []\n",
        "\n",
        "for idx, row in tqdm(df_clip.iterrows(), total=len(df_clip), desc=\"CLIP scoring\"):\n",
        "    img = load_image_safe(row[\"path\"], IMAGE_BASE)\n",
        "    if img is None:\n",
        "        sim_caption.append(np.nan)\n",
        "        sim_title.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    caption_text = str(row[\"image_caption\"])\n",
        "    title_text = str(row.get(\"item_name_flat\", \"\"))[:77]  # CLIP max 77 tokens\n",
        "\n",
        "    # Image vs Caption\n",
        "    inputs_cap = clip_processor(\n",
        "        text=[caption_text], images=[img], return_tensors=\"pt\", padding=True\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs_cap = clip_model(**inputs_cap)\n",
        "    sim_cap = outputs_cap.logits_per_image.item() / 100.0  # Normalize\n",
        "\n",
        "    # Image vs Title\n",
        "    inputs_title = clip_processor(\n",
        "        text=[title_text], images=[img], return_tensors=\"pt\", padding=True\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs_title = clip_model(**inputs_title)\n",
        "    sim_ttl = outputs_title.logits_per_image.item() / 100.0\n",
        "\n",
        "    sim_caption.append(sim_cap)\n",
        "    sim_title.append(sim_ttl)\n",
        "\n",
        "df_clip[\"clip_sim_caption\"] = sim_caption\n",
        "df_clip[\"clip_sim_title\"] = sim_title\n",
        "\n",
        "print(f\"\\n\\u2705 CLIP scoring complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Visualize CLIP similarities ---\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "valid_cap = df_clip[\"clip_sim_caption\"].dropna()\n",
        "valid_ttl = df_clip[\"clip_sim_title\"].dropna()\n",
        "\n",
        "axes[0].hist(valid_cap, bins=30, color=\"steelblue\", edgecolor=\"white\", alpha=0.8)\n",
        "axes[0].axvline(valid_cap.median(), color=\"red\", linestyle=\"--\",\n",
        "                label=f\"Median: {valid_cap.median():.3f}\")\n",
        "axes[0].set_xlabel(\"CLIP Cosine Similarity\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].set_title(\"Image vs. BLIP Caption\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(valid_ttl, bins=30, color=\"coral\", edgecolor=\"white\", alpha=0.8)\n",
        "axes[1].axvline(valid_ttl.median(), color=\"darkred\", linestyle=\"--\",\n",
        "                label=f\"Median: {valid_ttl.median():.3f}\")\n",
        "axes[1].set_xlabel(\"CLIP Cosine Similarity\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].set_title(\"Image vs. Product Title\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle(\"CLIP Image-Text Alignment Scores\", fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCLIP Similarity Summary ({clip_sample_size} products):\")\n",
        "print(f\"  Image vs Caption:  mean={valid_cap.mean():.3f}, median={valid_cap.median():.3f}\")\n",
        "print(f\"  Image vs Title:    mean={valid_ttl.mean():.3f}, median={valid_ttl.median():.3f}\")\n",
        "\n",
        "if valid_cap.median() > valid_ttl.median():\n",
        "    print(f\"  \\u2192 BLIP captions align BETTER with images than titles do \\u2713\")\n",
        "else:\n",
        "    print(f\"  \\u2192 Titles are more descriptive than BLIP captions for this dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Show best and worst caption matches ---\n",
        "\n",
        "df_clip_sorted = df_clip.dropna(subset=[\"clip_sim_caption\"]).sort_values(\n",
        "    \"clip_sim_caption\", ascending=False\n",
        ")\n",
        "\n",
        "print(\"\\u2550\" * 80)\n",
        "print(\"TOP 3 BEST CAPTION MATCHES (highest CLIP similarity)\")\n",
        "print(\"\\u2550\" * 80)\n",
        "for _, row in df_clip_sorted.head(3).iterrows():\n",
        "    img_path = IMAGE_BASE / str(row[\"path\"])\n",
        "    if img_path.exists():\n",
        "        display(IPImage(filename=str(img_path), width=180))\n",
        "    print(f\"  CLIP score: {row['clip_sim_caption']:.3f}\")\n",
        "    print(f\"  Caption:    {row['image_caption']}\")\n",
        "    print(f\"  Title:      {str(row.get('item_name_flat', ''))[:80]}\")\n",
        "    print(\"\\u2500\" * 80)\n",
        "\n",
        "print()\n",
        "print(\"\\u2550\" * 80)\n",
        "print(\"TOP 3 WORST CAPTION MATCHES (lowest CLIP similarity)\")\n",
        "print(\"\\u2550\" * 80)\n",
        "for _, row in df_clip_sorted.tail(3).iterrows():\n",
        "    img_path = IMAGE_BASE / str(row[\"path\"])\n",
        "    if img_path.exists():\n",
        "        display(IPImage(filename=str(img_path), width=180))\n",
        "    print(f\"  CLIP score: {row['clip_sim_caption']:.3f}\")\n",
        "    print(f\"  Caption:    {row['image_caption']}\")\n",
        "    print(f\"  Title:      {str(row.get('item_name_flat', ''))[:80]}\")\n",
        "    print(\"\\u2500\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations \\u2013 CLIP Analysis**\n",
        "\n",
        "- *Populate after running:*\n",
        "  - Are BLIP captions more aligned with images than raw titles?\n",
        "  - What do poorly-matched captions look like? (generic captions for niche products?)\n",
        "  - Implications for search: should we embed captions separately or concatenate with bullet_point?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6 – Export Enriched Dataset\n",
        "\n",
        "We merge captions back into the full DataFrame and export as `enriched_products.csv`.  \n",
        "This file is the input for the next notebook (`03_rag_prototype.ipynb`) and the production ingestion pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 6: Export Enriched Dataset\n",
        "# ============================================================\n",
        "\n",
        "# Merge captions back into the full DF\n",
        "# (df has all products; df_caption has the subset we captioned)\n",
        "\n",
        "# Drop old columns if re-running\n",
        "for col_drop in [\"image_caption\", \"detection_confidence\", \"detection_hit\"]:\n",
        "    if col_drop in df.columns:\n",
        "        df = df.drop(columns=[col_drop])\n",
        "\n",
        "# Merge captions + detection info\n",
        "merge_cols = [\"item_id\", \"image_caption\", \"detection_confidence\", \"detection_hit\"]\n",
        "merge_cols = [c for c in merge_cols if c in df_caption.columns]\n",
        "caption_map = df_caption[merge_cols].dropna(subset=[\"image_caption\"])\n",
        "df_enriched = df.merge(caption_map, on=\"item_id\", how=\"left\")\n",
        "\n",
        "# Build the enriched text field that will be embedded for RAG\n",
        "def build_enriched_text(row):\n",
        "    \"\"\"Concatenate available text fields into a single search document.\n",
        "\n",
        "    Priority: title + bullet_points + image_caption + keywords.\n",
        "    This is the text that will be embedded for semantic search.\n",
        "\n",
        "    Args:\n",
        "        row: DataFrame row.\n",
        "\n",
        "    Returns:\n",
        "        Concatenated text string.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for col in [\"item_name_flat\", \"bullet_point_flat\", \"image_caption\", \"item_keywords_flat\"]:\n",
        "        val = row.get(col)\n",
        "        if pd.notna(val) and str(val).strip() and str(val) != \"None\":\n",
        "            parts.append(str(val).strip())\n",
        "    return \" . \".join(parts) if parts else None\n",
        "\n",
        "\n",
        "df_enriched[\"enriched_text\"] = df_enriched.apply(build_enriched_text, axis=1)\n",
        "\n",
        "print(f\"Enriched DataFrame: {df_enriched.shape[0]:,} rows \\u00d7 {df_enriched.shape[1]} columns\")\n",
        "print(f\"\\nField coverage:\")\n",
        "for col in [\"image_caption\", \"detection_confidence\", \"enriched_text\", \"item_name_flat\", \"bullet_point_flat\"]:\n",
        "    if col in df_enriched.columns:\n",
        "        n = df_enriched[col].notna().sum()\n",
        "        print(f\"  {col:30s} {n:,} / {len(df_enriched):,} ({n/len(df_enriched):.1%})\")\n",
        "\n",
        "# Detection summary for enriched dataset\n",
        "if \"detection_hit\" in df_enriched.columns:\n",
        "    n_det = df_enriched[\"detection_hit\"].sum()\n",
        "    n_cap = df_enriched[\"image_caption\"].notna().sum()\n",
        "    print(f\"\\nDetection in enriched dataset:\")\n",
        "    print(f\"  OWL-ViT detected (cropped): {n_det:,} / {n_cap:,} ({n_det/max(n_cap,1):.1%})\")\n",
        "    print(f\"  Fallback (full image):      {n_cap - n_det:,} / {n_cap:,} ({(n_cap - n_det)/max(n_cap,1):.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Enriched text length distribution ---\n",
        "enriched_word_counts = df_enriched[\"enriched_text\"].dropna().apply(lambda x: len(x.split()))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "ax.hist(enriched_word_counts, bins=50, color=\"teal\", edgecolor=\"white\", alpha=0.8)\n",
        "ax.axvline(enriched_word_counts.median(), color=\"red\", linestyle=\"--\",\n",
        "           label=f\"Median: {enriched_word_counts.median():.0f} words\")\n",
        "ax.set_xlabel(\"Word Count\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title(\"Enriched Text Length (title + bullets + caption + keywords)\", fontsize=13)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Enriched text stats:\")\n",
        "print(f\"  Mean words:   {enriched_word_counts.mean():.0f}\")\n",
        "print(f\"  Median words: {enriched_word_counts.median():.0f}\")\n",
        "print(f\"  P25 / P75:    {enriched_word_counts.quantile(0.25):.0f} / {enriched_word_counts.quantile(0.75):.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# ============================================================\n",
        "# Save all outputs to /kaggle/working/ for the next notebook\n",
        "# ============================================================\n",
        "# (Same pattern as 01-shoptalk-eda.ipynb cell 49)\n",
        "\n",
        "# Define explicit output paths\n",
        "OUTPUT_DIR = '/kaggle/working/'\n",
        "\n",
        "ENRICHED_CSV_PATH = os.path.join(OUTPUT_DIR, 'enriched_products.csv')\n",
        "ENRICHED_PKL_PATH = os.path.join(OUTPUT_DIR, 'enriched_products.pkl')\n",
        "ENRICHED_FULL_PKL_PATH = os.path.join(OUTPUT_DIR, 'enriched_products_full.pkl')\n",
        "\n",
        "# Select columns for export (clean subset for downstream RAG)\n",
        "EXPORT_COLS = [\n",
        "    \"item_id\", \"item_name_flat\", \"bullet_point_flat\", \"item_keywords_flat\",\n",
        "    \"product_description_flat\", \"brand_flat\", \"color_flat\", \"product_type_flat\",\n",
        "    \"main_image_id\", \"path\", \"country\",\n",
        "    \"image_caption\", \"detection_confidence\", \"enriched_text\",\n",
        "]\n",
        "# Only include columns that exist\n",
        "export_cols = [c for c in EXPORT_COLS if c in df_enriched.columns]\n",
        "df_export = df_enriched[export_cols].copy()\n",
        "\n",
        "print(f\"Saving enriched dataset (Shape: {df_export.shape})...\")\n",
        "\n",
        "# 1. Save as Pickle (Recommended – preserves dtypes, lists, NaN)\n",
        "df_export.to_pickle(ENRICHED_PKL_PATH)\n",
        "print(f\"✓ Saved Pickle: {ENRICHED_PKL_PATH}\")\n",
        "\n",
        "# 2. Save as CSV (Backup for quick inspection)\n",
        "df_export.to_csv(ENRICHED_CSV_PATH, index=False, escapechar='\\\\')\n",
        "print(f\"✓ Saved CSV:    {ENRICHED_CSV_PATH}\")\n",
        "\n",
        "# 3. Save FULL enriched DF with all columns (for debugging / deeper analysis)\n",
        "df_enriched.to_pickle(ENRICHED_FULL_PKL_PATH)\n",
        "print(f\"✓ Saved Full Pickle: {ENRICHED_FULL_PKL_PATH} ({df_enriched.shape[1]} columns)\")\n",
        "\n",
        "# Verify files on disk\n",
        "print(f\"\\n✅ Export complete. Files in {OUTPUT_DIR}:\")\n",
        "for fpath in [ENRICHED_PKL_PATH, ENRICHED_CSV_PATH, ENRICHED_FULL_PKL_PATH]:\n",
        "    if os.path.exists(fpath):\n",
        "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "        print(f\"  {os.path.basename(fpath):35s} {size_mb:.1f} MB\")\n",
        "\n",
        "print(f\"\\n📋 Next notebook (03_rag_prototype) should load from:\")\n",
        "print(f\"   pd.read_pickle('{ENRICHED_PKL_PATH}')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Preview export ---\n",
        "df_export.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary & Key Findings\n",
        "\n",
        "### Detect-then-Caption Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Products captioned | *fill after run* |\n",
        "| Success rate | *fill after run* |\n",
        "| **OWL-ViT detection hit rate** | *fill after run* |\n",
        "| **Median detection confidence** | *fill after run* |\n",
        "| Median caption length | *fill after run* words |\n",
        "| Unique captions | *fill after run* |\n",
        "| Throughput | *fill after run* img/s |\n",
        "| Device | *fill after run* |\n",
        "\n",
        "### Quality Assessment\n",
        "\n",
        "1. **Detection Rate:** *(what % of products did OWL-ViT locate?)*\n",
        "2. **Caption Coverage:** *(what % of products got a caption?)*\n",
        "3. **Diversity:** *(are captions varied or repetitive for similar products?)*\n",
        "4. **Complementarity:** *(low Jaccard with bullets = captions add new visual info)*\n",
        "5. **CLIP Alignment:** *(higher sim for captions vs titles = captions describe what\\u2019s in the image)*\n",
        "\n",
        "### Implications for RAG Pipeline\n",
        "\n",
        "- **Enriched text** = `title + bullet_points + image_caption + keywords` (median ~X words).\n",
        "- Recommended **chunk size**: 256\\u2013512 tokens (based on enriched text length distribution).\n",
        "- Image captions **complement** bullet_point text (low vocabulary overlap).\n",
        "- Products with missing `product_description` (97.3%) now have visual text from captions.\n",
        "\n",
        "### Model Comparison (for Documentation)\n",
        "\n",
        "| Model | Task | Speed | Notes |\n",
        "|-------|------|-------|-------|\n",
        "| OWL-ViT-base | Object Detection | ~X img/s | Zero-shot product detection using title/category |\n",
        "| BLIP-base | Captioning | ~X img/s | Captions cropped product region |\n",
        "| CLIP-base | Similarity | ~X img/s | Validates caption quality |\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **`03_rag_prototype.ipynb`** (Local) \\u2013 Load `enriched_products.csv` into ChromaDB; test RAG retrieval.  \n",
        "2. **`04_voice_test.ipynb`** (Local) \\u2013 Validate Whisper STT.  \n",
        "3. **Production ingestion** (`backend/scripts/ingest_abo.py`) \\u2013 Use `enriched_text` for embeddings + `image_caption` as metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Final Summary (auto-fill)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SHOPTALK DETECT-THEN-CAPTION \\u2013 SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total Products:          {len(df_enriched):,}\")\n",
        "print(f\"Products Captioned:      {df_enriched['image_caption'].notna().sum():,}\")\n",
        "print(f\"Caption Success Rate:    {df_enriched['image_caption'].notna().mean():.1%}\")\n",
        "\n",
        "# Detection stats\n",
        "if \"detection_hit\" in df_enriched.columns:\n",
        "    n_cap = df_enriched[\"image_caption\"].notna().sum()\n",
        "    n_det = df_enriched[\"detection_hit\"].sum()\n",
        "    print(f\"OWL-ViT Detection Hits:  {n_det:,} / {n_cap:,} ({n_det/max(n_cap,1):.1%})\")\n",
        "    print(f\"Fallback (Full Image):   {n_cap - n_det:,} ({(n_cap - n_det)/max(n_cap,1):.1%})\")\n",
        "if \"detection_confidence\" in df_enriched.columns:\n",
        "    valid_c = df_enriched[\"detection_confidence\"].dropna()\n",
        "    if len(valid_c) > 0:\n",
        "        print(f\"Median Det. Confidence:  {valid_c.median():.3f}\")\n",
        "\n",
        "if 'caption_word_counts' in dir():\n",
        "    print(f\"Median Caption Words:    {caption_word_counts.median():.0f}\")\n",
        "if 'n_unique' in dir():\n",
        "    print(f\"Unique Captions:         {n_unique:,}\")\n",
        "print(f\"Enriched Text Coverage:  {df_enriched['enriched_text'].notna().mean():.1%}\")\n",
        "if 'enriched_word_counts' in dir():\n",
        "    print(f\"Median Enriched Words:   {enriched_word_counts.median():.0f}\")\n",
        "print(f\"Device Used:             {DEVICE} ({GPU_NAME})\")\n",
        "print(f\"Output Dir:              {OUTPUT_DIR}\")\n",
        "print(f\"Output Files:            enriched_products.pkl, enriched_products.csv\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n\\u2705 Detect-then-caption complete. Proceed to RAG prototype notebook.\")\n",
        "print(f\"   Load with: pd.read_pickle('{ENRICHED_PKL_PATH}')\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 296871536,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
