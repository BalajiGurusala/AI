{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ShopTalk – LLM Integration & End-to-End RAG Pipeline\n",
        "\n",
        "**Project:** ShopTalk – AI-Powered Shopping Assistant  \n",
        "**Dataset:** [Amazon Berkeley Objects (ABO)](https://amazon-berkeley-objects.s3.amazonaws.com/index.html)  \n",
        "**Author:** Balaji Gurusala  \n",
        "**Notebook Scope:** T000d+ from `.spec/tasks.md` – LLM Integration, RAG Generation, Model Comparison  \n",
        "**Prerequisite:** `03-rag-prototype.ipynb` must have been run (produces search indexes + config in `/kaggle/working/`)  \n",
        "**Environment:** Local MacBook (Apple Silicon / MPS) or Kaggle (CUDA) or CPU\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "\n",
        "Complete the **Generation** half of the RAG pipeline by integrating Large Language Models.  \n",
        "NB03 built the **Retrieval** engine (hybrid search, P@5 = 0.775). This notebook adds:\n",
        "\n",
        "| Component | Details |\n",
        "|-----------|--------|\n",
        "| **LLM Integration** | OpenAI GPT-4o-mini (primary) + Groq Llama-3.3-70B (open-source comparison) |\n",
        "| **Prompt Engineering** | System prompt, context formatting, few-shot examples |\n",
        "| **RAG Pipeline** | query → hybrid_search → context assembly → LLM generation → response |\n",
        "| **Model Comparison** | Side-by-side GPT-4o-mini vs Llama-3.3 on quality, latency, cost |\n",
        "| **Evaluation** | RAGAS-style metrics: Faithfulness, Answer Relevance, Latency |\n",
        "| **Synthetic Prices** | Category-based price generation for filter demo |\n",
        "| **Session Context** | Multi-turn conversation support for follow-up queries |\n",
        "\n",
        "### Inputs (from NB03)\n",
        "\n",
        "| Artifact | Description |\n",
        "|----------|-------------|\n",
        "| `rag_products.pkl` | Full product catalogue (9,190 rows × 51 cols) |\n",
        "| `rag_text_index.npy` | Pre-normalised SentenceTransformer embeddings (9190 × 384) |\n",
        "| `rag_image_index.npy` | Pre-normalised CLIP image embeddings (9190 × 512) |\n",
        "| `rag_config.json` | Hyperparameters, model IDs, benchmark results |\n",
        "\n",
        "### Outputs\n",
        "\n",
        "| Artifact | Description |\n",
        "|----------|-------------|\n",
        "| `llm_config.json` | LLM settings, prompt templates, evaluation results |\n",
        "| `llm_evaluation.csv` | Detailed evaluation results for 50+ queries |\n",
        "| `products_with_prices.pkl` | Products with synthetic prices for UI demo |\n",
        "| `streamlit_app/` | Production-ready Streamlit app code |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 0 – Environment Setup & Hardware Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 0: Environment Setup\n",
        "# ============================================================\n",
        "import sys, os, json, time, re, warnings, hashlib\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Dict, Tuple, Any\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(f\"Python {sys.version}\")\n",
        "\n",
        "# --- Hardware Detection (per constitution.md) ---\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DEVICE = (\n",
        "    \"cuda\" if torch.cuda.is_available() else\n",
        "    \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else\n",
        "    \"cpu\"\n",
        ")\n",
        "GPU_NAME = torch.cuda.get_device_name(0) if DEVICE == \"cuda\" else DEVICE\n",
        "print(f\"PyTorch {torch.__version__}\")\n",
        "print(f\"Device: {DEVICE} ({GPU_NAME})\")\n",
        "\n",
        "# --- Platform Detection ---\n",
        "ON_KAGGLE = Path(\"/kaggle/working\").exists()\n",
        "print(f\"Platform: {'Kaggle' if ON_KAGGLE else 'Local'}\")\n",
        "\n",
        "# --- Install extra deps if needed ---\n",
        "def _install(*pkgs):\n",
        "    import subprocess\n",
        "    for pkg in pkgs:\n",
        "        try:\n",
        "            __import__(pkg.split(\"==\")[0].replace(\"-\", \"_\"))\n",
        "        except ImportError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "_install(\"langchain\", \"langchain-core\", \"langchain-openai\", \"langchain-groq\",\n",
        "         \"langchain-ollama\", \"openai\", \"groq\", \"python-dotenv\", \"tiktoken\", \"httpx\",\n",
        "         \"chromadb\")\n",
        "\n",
        "print(\"\\n\\u2713 Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1 – Load NB03 Artifacts & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 1: Load NB03 artifacts\n",
        "# ============================================================\n",
        "\n",
        "# --- Resolve data directory ---\n",
        "# On Kaggle: NB03 output is attached as a dataset version\n",
        "# Locally: look in ../data/ or current directory\n",
        "_candidates = [\n",
        "    Path(\"/kaggle/input/shoptalk-rag-prototype\"),   # Kaggle dataset\n",
        "    Path(\"/kaggle/working\"),                         # Same-session Kaggle\n",
        "    Path(\"../data\"),                                 # Local dev\n",
        "    Path(\".\"),                                       # Fallback\n",
        "]\n",
        "DATA_DIR = None\n",
        "for d in _candidates:\n",
        "    if (d / \"rag_products.pkl\").exists():\n",
        "        DATA_DIR = d\n",
        "        break\n",
        "assert DATA_DIR is not None, (\n",
        "    f\"Cannot find rag_products.pkl in any of: {[str(d) for d in _candidates]}. \"\n",
        "    \"Run 03-rag-prototype.ipynb first.\"\n",
        ")\n",
        "\n",
        "EXPORT_DIR = Path(\"/kaggle/working\") if ON_KAGGLE else Path(\"../data\")\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data directory:   {DATA_DIR}\")\n",
        "print(f\"Export directory:  {EXPORT_DIR}\")\n",
        "\n",
        "# --- Load products ---\n",
        "df = pd.read_pickle(DATA_DIR / \"rag_products.pkl\")\n",
        "print(f\"\\nProducts loaded: {len(df):,} rows x {len(df.columns)} cols\")\n",
        "\n",
        "# --- Load embedding indexes ---\n",
        "TEXT_INDEX = np.load(DATA_DIR / \"rag_text_index.npy\")\n",
        "IMAGE_INDEX = np.load(DATA_DIR / \"rag_image_index.npy\")\n",
        "print(f\"Text index:   {TEXT_INDEX.shape} (dtype={TEXT_INDEX.dtype})\")\n",
        "print(f\"Image index:  {IMAGE_INDEX.shape} (dtype={IMAGE_INDEX.dtype})\")\n",
        "\n",
        "# --- Load RAG config ---\n",
        "with open(DATA_DIR / \"rag_config.json\") as f:\n",
        "    RAG_CONFIG = json.load(f)\n",
        "print(f\"RAG config:   {list(RAG_CONFIG.keys())}\")\n",
        "\n",
        "# --- Verify alignment ---\n",
        "assert len(df) == TEXT_INDEX.shape[0] == IMAGE_INDEX.shape[0], \"Index/DataFrame size mismatch!\"\n",
        "print(f\"\\n\\u2713 All artifacts loaded and aligned ({len(df):,} products)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 1b: Generate Synthetic Prices (ABO has no price field)\n",
        "# ============================================================\n",
        "# Per data-model.md: \"ABO has no price field; synthetic prices\n",
        "# assigned during ingestion if price filtering is needed.\"\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Price ranges by product category (realistic for demo)\n",
        "CATEGORY_PRICE_RANGES = {\n",
        "    \"SHOES\":                  (29.99, 149.99),\n",
        "    \"SHIRT\":                  (14.99, 59.99),\n",
        "    \"T_SHIRT\":                (9.99,  39.99),\n",
        "    \"CELLULAR_PHONE_CASE\":    (7.99,  39.99),\n",
        "    \"HOME\":                   (12.99, 89.99),\n",
        "    \"HOME_BED_AND_BATH\":      (14.99, 79.99),\n",
        "    \"HARDWARE\":               (4.99,  49.99),\n",
        "    \"HARDWARE_HANDLE\":        (5.99,  29.99),\n",
        "    \"THERMOPLASTIC_FILAMENT\": (15.99, 45.99),\n",
        "    \"FURNITURE\":              (49.99, 399.99),\n",
        "    \"OTTOMAN\":                (39.99, 199.99),\n",
        "    \"CHAIR\":                  (59.99, 299.99),\n",
        "    \"TABLE\":                  (49.99, 349.99),\n",
        "    \"LIGHTING\":               (14.99, 89.99),\n",
        "    \"LAMP\":                   (19.99, 99.99),\n",
        "    \"WATCH\":                  (24.99, 199.99),\n",
        "    \"LUGGAGE\":                (29.99, 149.99),\n",
        "    \"FINERING\":               (9.99,  89.99),\n",
        "}\n",
        "DEFAULT_RANGE = (9.99, 99.99)\n",
        "\n",
        "def generate_price(category: str) -> float:\n",
        "    \"\"\"Generate a realistic synthetic price based on product category.\"\"\"\n",
        "    low, high = CATEGORY_PRICE_RANGES.get(str(category).upper(), DEFAULT_RANGE)\n",
        "    price = np.random.uniform(low, high)\n",
        "    return round(price, 2)\n",
        "\n",
        "if \"price\" not in df.columns or df[\"price\"].isna().all():\n",
        "    df[\"price\"] = df[\"product_type_flat\"].apply(\n",
        "        lambda c: generate_price(str(c).split(\"/\")[0] if pd.notna(c) else \"\")\n",
        "    )\n",
        "    print(f\"\\u2713 Synthetic prices generated for {len(df):,} products\")\n",
        "    print(f\"  Price range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
        "    print(f\"  Mean: ${df['price'].mean():.2f} | Median: ${df['price'].median():.2f}\")\n",
        "else:\n",
        "    print(f\"\\u2713 Prices already present\")\n",
        "\n",
        "# Quick distribution by top categories\n",
        "print(\"\\nPrice by top categories:\")\n",
        "for cat in df[\"product_type_flat\"].value_counts().head(8).index:\n",
        "    subset = df[df[\"product_type_flat\"] == cat][\"price\"]\n",
        "    print(f\"  {cat:30s}  ${subset.mean():6.2f} avg  (n={len(subset)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2 – Load Search Models (SentenceTransformer + CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 2: Load Search Models (loaded once per constitution.md)\n",
        "# ============================================================\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "ST_MODEL_ID = RAG_CONFIG.get(\"text_model_id\", \"all-MiniLM-L6-v2\")\n",
        "CLIP_MODEL_ID = RAG_CONFIG.get(\"image_model_id\", \"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(f\"Loading SentenceTransformer: {ST_MODEL_ID}\")\n",
        "t0 = time.time()\n",
        "st_model = SentenceTransformer(ST_MODEL_ID, device=DEVICE)\n",
        "print(f\"  \\u2713 Loaded in {time.time()-t0:.1f}s\")\n",
        "\n",
        "print(f\"Loading CLIP: {CLIP_MODEL_ID}\")\n",
        "t0 = time.time()\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE).eval()\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
        "print(f\"  \\u2713 Loaded in {time.time()-t0:.1f}s\")\n",
        "\n",
        "print(f\"\\n\\u2713 All search models loaded on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3 – Load Search Pipeline + ChromaDB\n",
        "\n",
        "Instead of copy-pasting NB03's search code, we:\n",
        "1. **Import** the shared `src/search.py` module (single source of truth for search logic)\n",
        "2. **Load ChromaDB** from NB03's persisted directory (if available) for Stage-1 retrieval\n",
        "3. **Fall back** to in-memory NumPy if ChromaDB isn't found\n",
        "\n",
        "Only the **query encoding** models (loaded in Step 2) and the **shared search module** are needed.  \n",
        "The product embeddings are already indexed — either in ChromaDB or the `.npy` files loaded in Step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 3: Search Pipeline + ChromaDB\n",
        "# ============================================================\n",
        "# LOCAL: imports from src/search.py (single source of truth)\n",
        "# KAGGLE: defines everything inline (self-contained, zero setup)\n",
        "\n",
        "import re\n",
        "\n",
        "# ---------- try the shared module first ----------\n",
        "_SEARCH_IMPORTED = False\n",
        "try:\n",
        "    project_root = Path(\"..\").resolve()\n",
        "    if str(project_root) not in sys.path:\n",
        "        sys.path.insert(0, str(project_root))\n",
        "    from src.search import (\n",
        "        hybrid_search, l2_normalize, apply_rerank,\n",
        "        compute_dynamic_alpha, retrieve_inmemory,\n",
        "    )\n",
        "    _SEARCH_IMPORTED = True\n",
        "    print(\"\\u2713 Imported search pipeline from src/search.py (local)\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# ---------- inline fallback (Kaggle / Colab) ----------\n",
        "if not _SEARCH_IMPORTED:\n",
        "    print(\"\\u2139 src/search.py not found — defining search inline (Kaggle mode)\")\n",
        "\n",
        "    # --- constants ---\n",
        "    ALPHA_DEFAULT_CFG = RAG_CONFIG.get(\"alpha_default\", 0.6)\n",
        "    LEXICAL_WEIGHT  = 0.16\n",
        "    TITLE_WEIGHT    = 0.10\n",
        "    TYPE_WEIGHT     = 0.06\n",
        "    HEAD_NOUN_MISS_PENALTY = 0.50\n",
        "    GENDER_MISS_PENALTY    = 0.40\n",
        "    LOW_CONFIDENCE_CUTOFF  = 0.30\n",
        "\n",
        "    QUERY_STOPWORDS = {\n",
        "        \"for\",\"with\",\"and\",\"the\",\"a\",\"an\",\"in\",\"on\",\"to\",\"of\",\"by\",\"from\",\n",
        "        \"best\",\"good\",\"new\",\"comfortable\",\"great\",\"nice\",\"high\",\"quality\",\n",
        "        \"s\",\"under\",\"below\",\"above\",\"less\",\"than\",\"more\",\"about\",\n",
        "        \"show\",\"me\",\"find\",\"get\",\"want\",\"need\",\"looking\",\"search\",\n",
        "    }\n",
        "    FEMALE_TOKENS = {\"women\",\"woman\",\"female\",\"ladies\",\"lady\",\"girls\",\"girl\",\"womens\"}\n",
        "    MALE_TOKENS   = {\"men\",\"man\",\"male\",\"mens\",\"boys\",\"boy\"}\n",
        "    ABO_CATEGORY_HINTS = {\n",
        "        \"shoe\":{\"SHOES\"},\"shoes\":{\"SHOES\"},\"sneaker\":{\"SHOES\"},\"sneakers\":{\"SHOES\"},\n",
        "        \"boot\":{\"SHOES\"},\"boots\":{\"SHOES\"},\"sandal\":{\"SHOES\"},\"sandals\":{\"SHOES\"},\n",
        "        \"filament\":{\"THERMOPLASTIC_FILAMENT\",\"MECHANICAL_COMPONENTS\"},\n",
        "        \"pla\":{\"THERMOPLASTIC_FILAMENT\"},\"abs\":{\"THERMOPLASTIC_FILAMENT\"},\n",
        "        \"3d\":{\"THERMOPLASTIC_FILAMENT\"},\"printer\":{\"THERMOPLASTIC_FILAMENT\"},\n",
        "        \"phone\":{\"CELLULAR_PHONE_CASE\"},\"case\":{\"CELLULAR_PHONE_CASE\"},\n",
        "        \"cover\":{\"CELLULAR_PHONE_CASE\"},\n",
        "        \"drawer\":{\"HARDWARE\"},\"slides\":{\"HARDWARE\"},\"slide\":{\"HARDWARE\"},\n",
        "        \"handle\":{\"HARDWARE_HANDLE\"},\"hardware\":{\"HARDWARE\",\"HARDWARE_HANDLE\"},\n",
        "        \"shirt\":{\"SHIRT\",\"T_SHIRT\"},\"tshirt\":{\"SHIRT\",\"T_SHIRT\"},\n",
        "        \"t-shirt\":{\"SHIRT\",\"T_SHIRT\"},\"polo\":{\"SHIRT\"},\n",
        "        \"pillow\":{\"HOME_BED_AND_BATH\",\"HOME\"},\"sheet\":{\"HOME_BED_AND_BATH\"},\n",
        "        \"curtain\":{\"HOME_BED_AND_BATH\"},\"towel\":{\"HOME_BED_AND_BATH\"},\n",
        "        \"ottoman\":{\"OTTOMAN\",\"FURNITURE\"},\"chair\":{\"CHAIR\",\"FURNITURE\"},\n",
        "        \"table\":{\"TABLE\",\"FURNITURE\"},\"lamp\":{\"LIGHTING\",\"LAMP\"},\n",
        "        \"watch\":{\"WATCH\"},\"backpack\":{\"LUGGAGE\"},\n",
        "    }\n",
        "    VISUAL_CUES    = {\"colorful\",\"patterned\",\"floral\",\"striped\",\"printed\",\"design\",\n",
        "                      \"aesthetic\",\"stylish\",\"cute\",\"pretty\",\"beautiful\"}\n",
        "    TECHNICAL_CUES = {\"inch\",\"mm\",\"kg\",\"watt\",\"volt\",\"capacity\",\"specs\",\n",
        "                      \"compatible\",\"mount\",\"gauge\",\"thread\",\"count\"}\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _tok(t):\n",
        "        t = re.sub(r\"[^a-zA-Z0-9\\-\\s]\",\" \",str(t).lower())\n",
        "        return [w for w in t.split() if w and w not in QUERY_STOPWORDS]\n",
        "\n",
        "    def _overlap(q, t):\n",
        "        qt = set(_tok(q))\n",
        "        return len(qt & set(_tok(t)))/max(1,len(qt)) if qt else 0.0\n",
        "\n",
        "    def _expand(tokens):\n",
        "        e = set(tokens)\n",
        "        for t in tokens:\n",
        "            if \"-\" in t:\n",
        "                for p in t.split(\"-\"):\n",
        "                    if p and p in ABO_CATEGORY_HINTS: e.add(p)\n",
        "        return e\n",
        "\n",
        "    def l2_normalize(x):\n",
        "        n = np.linalg.norm(x, axis=1, keepdims=True)\n",
        "        return x / np.where(n==0, 1, n)\n",
        "\n",
        "    def compute_dynamic_alpha(query):\n",
        "        q = set(_tok(query))\n",
        "        a = ALPHA_DEFAULT_CFG\n",
        "        v = len(q & VISUAL_CUES)\n",
        "        t = len(q & TECHNICAL_CUES)\n",
        "        n = bool(re.search(r\"\\d\", query))\n",
        "        if v: a -= 0.15*min(v,2)\n",
        "        if t or n: a += 0.10*min(t+int(n),2)\n",
        "        return max(0.2, min(0.9, a))\n",
        "\n",
        "    def apply_rerank(results, query, top_k):\n",
        "        if results.empty: return results\n",
        "        hn = _expand({t for t in _tok(query) if t in ABO_CATEGORY_HINTS})\n",
        "        exp_types = set()\n",
        "        for t in _tok(query): exp_types |= ABO_CATEGORY_HINTS.get(t, set())\n",
        "        q_g = set(_tok(query))\n",
        "        gi = \"female\" if (q_g & FEMALE_TOKENS) and not (q_g & MALE_TOKENS) else \\\n",
        "             \"male\"   if (q_g & MALE_TOKENS)   and not (q_g & FEMALE_TOKENS) else None\n",
        "        adj = results.copy()\n",
        "        adj[\"lex\"] = adj.apply(lambda r: _overlap(query, \" \".join([\n",
        "            str(r.get(\"item_name_flat\",\"\")), str(r.get(\"enriched_text\",\"\")),\n",
        "            str(r.get(\"product_type_flat\",\"\"))])), axis=1)\n",
        "        adj[\"tol\"] = adj.apply(lambda r: _overlap(query, str(r.get(\"item_name_flat\",\"\"))), axis=1)\n",
        "        adj[\"tyo\"] = adj.apply(lambda r: _overlap(query, str(r.get(\"product_type_flat\",\"\"))), axis=1)\n",
        "        adj[\"fs\"] = adj[\"hybrid_score\"] + LEXICAL_WEIGHT*adj[\"lex\"] + TITLE_WEIGHT*adj[\"tol\"] + TYPE_WEIGHT*adj[\"tyo\"]\n",
        "        hm, gm = [], []\n",
        "        for _, row in adj.iterrows():\n",
        "            hay = \" \".join([str(row.get(\"item_name_flat\",\"\")),str(row.get(\"enriched_text\",\"\")),\n",
        "                            str(row.get(\"product_type_flat\",\"\"))]).lower()\n",
        "            ht = set(_tok(hay)); he = _expand(ht)\n",
        "            h = 1.0\n",
        "            if hn and not (hn & he):\n",
        "                pt = str(row.get(\"product_type_flat\",\"\")).upper()\n",
        "                if not (exp_types and any(t in pt for t in exp_types)):\n",
        "                    h = 1.0 - HEAD_NOUN_MISS_PENALTY\n",
        "            g = 1.0\n",
        "            if gi==\"female\" and (ht & MALE_TOKENS) and not (ht & FEMALE_TOKENS): g=1.0-GENDER_MISS_PENALTY\n",
        "            elif gi==\"male\" and (ht & FEMALE_TOKENS) and not (ht & MALE_TOKENS): g=1.0-GENDER_MISS_PENALTY\n",
        "            hm.append(h); gm.append(g)\n",
        "        adj[\"fs\"] *= np.array(hm)*np.array(gm)\n",
        "        adj = adj.sort_values(\"fs\", ascending=False).reset_index(drop=True)\n",
        "        strong = adj[adj[\"fs\"]>=LOW_CONFIDENCE_CUTOFF]\n",
        "        out = strong.head(top_k).copy() if len(strong)>=top_k else adj.head(top_k).copy()\n",
        "        out[\"hybrid_score\"] = out[\"fs\"]; out[\"_rank\"] = np.arange(1,len(out)+1)\n",
        "        return out\n",
        "\n",
        "    def hybrid_search(query, df, text_index, image_index, encode_text_fn,\n",
        "                      encode_clip_fn, top_k=5, alpha=None, price_max=None,\n",
        "                      category=None, rerank=True, **kw):\n",
        "        if alpha is None: alpha = compute_dynamic_alpha(query) if rerank else ALPHA_DEFAULT_CFG\n",
        "        qt = encode_text_fn(query); qc = encode_clip_fn(query)\n",
        "        ts = (text_index @ qt.T).squeeze(); ims = (image_index @ qc.T).squeeze()\n",
        "        scores = alpha*ts + (1-alpha)*ims\n",
        "        nf = min(len(df), max(top_k*12, 80))\n",
        "        ti = np.argsort(scores)[::-1][:nf]\n",
        "        res = df.iloc[ti].copy(); res[\"hybrid_score\"] = scores[ti]; res = res.reset_index(drop=True)\n",
        "        if price_max is not None and \"price\" in res.columns:\n",
        "            res = res[res[\"price\"]<=price_max].reset_index(drop=True)\n",
        "        if category and \"product_type_flat\" in res.columns:\n",
        "            res = res[res[\"product_type_flat\"].str.upper().str.contains(category.upper(),na=False)].reset_index(drop=True)\n",
        "        if res.empty: return res\n",
        "        res.attrs[\"alpha_used\"] = alpha\n",
        "        if rerank: res = apply_rerank(res, query, top_k)\n",
        "        else: res = res.sort_values(\"hybrid_score\",ascending=False).head(top_k).copy(); res[\"_rank\"]=np.arange(1,len(res)+1)\n",
        "        res.attrs[\"alpha_used\"] = alpha\n",
        "        return res\n",
        "\n",
        "    print(\"  \\u2713 Inline search pipeline defined (all functions available)\")\n",
        "\n",
        "# ============================================================\n",
        "# Query encoder closures (capture the loaded models from Step 2)\n",
        "# ============================================================\n",
        "def encode_text(query: str) -> np.ndarray:\n",
        "    \"\"\"Encode query with SentenceTransformer (L2-normalised).\"\"\"\n",
        "    emb = st_model.encode([query], show_progress_bar=False, normalize_embeddings=True)\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "def encode_clip(query: str) -> np.ndarray:\n",
        "    \"\"\"Encode query with CLIP text encoder (L2-normalised).\"\"\"\n",
        "    inputs = clip_processor(\n",
        "        text=[query], return_tensors=\"pt\", padding=True, truncation=True\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_text_features(**inputs)\n",
        "    emb = features.cpu().numpy().astype(np.float32)\n",
        "    return l2_normalize(emb)\n",
        "\n",
        "# ============================================================\n",
        "# Try to connect to NB03's persisted ChromaDB\n",
        "# ============================================================\n",
        "TEXT_COLLECTION = None\n",
        "IMAGE_COLLECTION = None\n",
        "ITEM_ID_TO_IDX = None\n",
        "\n",
        "chroma_candidates = [\n",
        "    DATA_DIR / \"chroma_db\",\n",
        "    DATA_DIR / \"..\" / \"chroma_db\",\n",
        "    Path(\"/kaggle/input/shoptalk-rag-prototype/chroma_db\"),\n",
        "]\n",
        "\n",
        "for chroma_path in chroma_candidates:\n",
        "    if chroma_path.exists() and any(chroma_path.iterdir()):\n",
        "        try:\n",
        "            import chromadb\n",
        "            chroma_client = chromadb.PersistentClient(path=str(chroma_path))\n",
        "            TEXT_COLLECTION = chroma_client.get_collection(\"products_text_v1\")\n",
        "            IMAGE_COLLECTION = chroma_client.get_collection(\"products_image_v1\")\n",
        "            ITEM_ID_TO_IDX = {str(row[\"item_id\"]): i for i, row in df.iterrows()}\n",
        "            print(f\"\\u2713 Connected to ChromaDB at {chroma_path}\")\n",
        "            print(f\"  Text collection:  {TEXT_COLLECTION.count():,} docs\")\n",
        "            print(f\"  Image collection: {IMAGE_COLLECTION.count():,} docs\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u26a0 ChromaDB at {chroma_path} failed: {e}\")\n",
        "\n",
        "if TEXT_COLLECTION is None:\n",
        "    print(\"\\u2139 ChromaDB not found — using in-memory NumPy backend (from .npy files)\")\n",
        "\n",
        "# ============================================================\n",
        "# Convenience wrapper — binds loaded data/models so callers\n",
        "# only need to pass the query string\n",
        "# ============================================================\n",
        "def search(\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    price_max: float = None,\n",
        "    category: str = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Search products. Uses ChromaDB if available, else in-memory NumPy.\"\"\"\n",
        "    return hybrid_search(\n",
        "        query=query,\n",
        "        df=df,\n",
        "        text_index=TEXT_INDEX,\n",
        "        image_index=IMAGE_INDEX,\n",
        "        encode_text_fn=encode_text,\n",
        "        encode_clip_fn=encode_clip,\n",
        "        top_k=top_k,\n",
        "        price_max=price_max,\n",
        "        category=category,\n",
        "        text_collection=TEXT_COLLECTION,\n",
        "        image_collection=IMAGE_COLLECTION,\n",
        "        item_id_to_idx=ITEM_ID_TO_IDX,\n",
        "    )\n",
        "\n",
        "# --- Smoke test ---\n",
        "test_results = search(\"red shoes for women\", top_k=3)\n",
        "print(f\"\\n\\u2713 search() working | Test query returned {len(test_results)} results\")\n",
        "for _, r in test_results.iterrows():\n",
        "    print(f\"  [{int(r['_rank'])}] {r['item_name_flat'][:70]:70s}  type={r['product_type_flat']}  ${r.get('price', 0):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4 – LLM Setup & API Configuration\n",
        "\n",
        "Per `constitution.md`: *\"LLM: Comparative Study — Proprietary: GPT-4o. Open Source: Llama 3 (via Ollama or Groq).\"*\n",
        "\n",
        "| Provider | Model | Where it runs | Setup |\n",
        "|----------|-------|---------------|-------|\n",
        "| **Ollama** (default local) | `llama3.2` / `llama3.1` | Your Mac/PC | `brew install ollama && ollama pull llama3.2` |\n",
        "| **OpenAI** (proprietary benchmark) | `gpt-4o-mini` | OpenAI cloud | API key in `.env` |\n",
        "| **Groq** (Kaggle fallback) | `llama-3.3-70b-versatile` | Groq cloud | API key in Kaggle Secrets |\n",
        "\n",
        "**Priority order:** Ollama (free, local) → OpenAI → Groq.  \n",
        "On **Kaggle**, Ollama is unavailable (no local server), so it falls back to Groq/OpenAI automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 4: LLM Client Setup (LangChain orchestration)\n",
        "# ============================================================\n",
        "# Priority: Ollama (free, local) → OpenAI → Groq\n",
        "# On Kaggle: Ollama unavailable, falls back to Groq/OpenAI\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- API Key Setup ---\n",
        "if ON_KAGGLE:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    secrets = UserSecretsClient()\n",
        "    OPENAI_API_KEY = secrets.get_secret(\"OPENAI_API_KEY\")\n",
        "    GROQ_API_KEY = secrets.get_secret(\"GROQ_API_KEY\")\n",
        "else:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")\n",
        "\n",
        "# --- Initialize LLM registry ---\n",
        "llm_registry = {}\n",
        "\n",
        "# ---- 1. Ollama (local, free — default for local dev) ----\n",
        "# Requires: brew install ollama && ollama pull llama3.2\n",
        "# Ollama runs a local server at http://localhost:11434\n",
        "if not ON_KAGGLE:\n",
        "    try:\n",
        "        from langchain_ollama import ChatOllama\n",
        "        import httpx\n",
        "\n",
        "        # Check if Ollama server is running\n",
        "        _ollama_ok = False\n",
        "        try:\n",
        "            resp = httpx.get(\"http://localhost:11434/api/tags\", timeout=3.0)\n",
        "            if resp.status_code == 200:\n",
        "                available_models = [m[\"name\"] for m in resp.json().get(\"models\", [])]\n",
        "                _ollama_ok = len(available_models) > 0\n",
        "                if _ollama_ok:\n",
        "                    print(f\"\\u2713 Ollama server running — models: {available_models[:5]}\")\n",
        "        except (httpx.ConnectError, httpx.TimeoutException):\n",
        "            pass\n",
        "\n",
        "        if _ollama_ok:\n",
        "            # Pick the best available Llama model\n",
        "            OLLAMA_MODEL = None\n",
        "            for candidate in [\"llama3.2\", \"llama3.1\", \"llama3\", \"llama2\", \"mistral\", \"phi3\"]:\n",
        "                matching = [m for m in available_models if candidate in m]\n",
        "                if matching:\n",
        "                    OLLAMA_MODEL = matching[0]\n",
        "                    break\n",
        "\n",
        "            if OLLAMA_MODEL is None and available_models:\n",
        "                OLLAMA_MODEL = available_models[0]  # Use whatever is available\n",
        "\n",
        "            if OLLAMA_MODEL:\n",
        "                llm_ollama = ChatOllama(\n",
        "                    model=OLLAMA_MODEL,\n",
        "                    temperature=0.3,\n",
        "                    num_predict=512,\n",
        "                )\n",
        "                llm_registry[f\"ollama/{OLLAMA_MODEL}\"] = llm_ollama\n",
        "                print(f\"\\u2713 Ollama/{OLLAMA_MODEL} initialized (local, free)\")\n",
        "        else:\n",
        "            print(\"\\u2139 Ollama server not running — skipping (start with: ollama serve)\")\n",
        "    except ImportError:\n",
        "        print(\"\\u2139 langchain-ollama not installed — run: pip install langchain-ollama\")\n",
        "else:\n",
        "    print(\"\\u2139 Ollama not available on Kaggle (no local server)\")\n",
        "\n",
        "# ---- 2. OpenAI GPT-4o-mini (proprietary benchmark) ----\n",
        "if OPENAI_API_KEY:\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        llm_gpt4o_mini = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            api_key=OPENAI_API_KEY,\n",
        "            temperature=0.3,\n",
        "            max_tokens=512,\n",
        "            request_timeout=30,\n",
        "        )\n",
        "        llm_registry[\"gpt-4o-mini\"] = llm_gpt4o_mini\n",
        "        print(\"\\u2713 GPT-4o-mini initialized (OpenAI)\")\n",
        "    except ImportError:\n",
        "        print(\"\\u26a0 langchain-openai not installed — run: pip install langchain-openai\")\n",
        "else:\n",
        "    print(\"\\u2139 OPENAI_API_KEY not set — GPT-4o-mini skipped\")\n",
        "\n",
        "# ---- 3. Groq Llama-3.3 (cloud, free tier — Kaggle fallback) ----\n",
        "if GROQ_API_KEY:\n",
        "    try:\n",
        "        from langchain_groq import ChatGroq\n",
        "        llm_llama_groq = ChatGroq(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            api_key=GROQ_API_KEY,\n",
        "            temperature=0.3,\n",
        "            max_tokens=512,\n",
        "        )\n",
        "        llm_registry[\"groq/llama-3.3-70b\"] = llm_llama_groq\n",
        "        print(\"\\u2713 Llama-3.3-70B initialized (Groq cloud)\")\n",
        "    except ImportError:\n",
        "        print(\"\\u26a0 langchain-groq not installed — run: pip install langchain-groq\")\n",
        "else:\n",
        "    print(\"\\u2139 GROQ_API_KEY not set — Groq skipped\")\n",
        "\n",
        "# --- Select default LLM ---\n",
        "# Priority: Ollama (free) → OpenAI (quality) → Groq (Kaggle fallback)\n",
        "if not llm_registry:\n",
        "    msg = \"No LLM available!\\n\"\n",
        "    if ON_KAGGLE:\n",
        "        msg += (\n",
        "            \"  KAGGLE SETUP REQUIRED:\\n\"\n",
        "            \"    1. Click 'Add-ons' → 'Secrets' in the right sidebar\\n\"\n",
        "            \"    2. Add secret: GROQ_API_KEY  (free at console.groq.com)\\n\"\n",
        "            \"       OR:        OPENAI_API_KEY (paid at platform.openai.com)\\n\"\n",
        "            \"    3. Toggle the secret ON for this notebook\\n\"\n",
        "            \"    4. Re-run this cell\"\n",
        "        )\n",
        "    else:\n",
        "        msg += (\n",
        "            \"  LOCAL:  Install Ollama → brew install ollama && ollama pull llama3.2 && ollama serve\\n\"\n",
        "            \"  CLOUD:  Set OPENAI_API_KEY or GROQ_API_KEY in .env\"\n",
        "        )\n",
        "    raise RuntimeError(msg)\n",
        "\n",
        "# Pick default: prefer Ollama for local, OpenAI for quality, Groq as fallback\n",
        "_priority = [\"ollama/\", \"gpt-4o-mini\", \"groq/\"]\n",
        "DEFAULT_LLM_NAME = list(llm_registry.keys())[0]  # fallback\n",
        "for prefix in _priority:\n",
        "    matches = [k for k in llm_registry if k.startswith(prefix)]\n",
        "    if matches:\n",
        "        DEFAULT_LLM_NAME = matches[0]\n",
        "        break\n",
        "\n",
        "DEFAULT_LLM = llm_registry[DEFAULT_LLM_NAME]\n",
        "print(f\"\\n\\u2705 Default LLM: {DEFAULT_LLM_NAME}\")\n",
        "print(f\"   Available:   {list(llm_registry.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5 – Prompt Engineering\n",
        "\n",
        "Designing the system prompt, context formatting, and few-shot templates  \n",
        "for the shopping assistant persona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 5: Prompt Engineering\n",
        "# ============================================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are ShopTalk, a friendly and knowledgeable AI shopping assistant.\n",
        "Your job is to help customers find products from our catalog and answer questions about them.\n",
        "\n",
        "RULES:\n",
        "1. ONLY recommend products from the provided context — never invent products or details.\n",
        "2. Be concise and helpful: aim for 2-4 sentences per recommendation.\n",
        "3. Highlight relevant features that match the customer's query (color, material, brand, size, etc.).\n",
        "4. When showing multiple products, briefly explain WHY each is relevant to the query.\n",
        "5. If no products match the query well, honestly say so and suggest trying different keywords.\n",
        "6. Use product IDs when referencing products so the UI can display product cards.\n",
        "7. Use a warm, conversational tone — like a helpful store associate.\n",
        "8. For follow-up questions, use conversation history for context.\n",
        "9. When prices are available, mention them to help the customer.\n",
        "10. Keep responses under 150 words unless the customer asks for details.\n",
        "\n",
        "FORMAT:\n",
        "- Lead with a brief, friendly answer to the question.\n",
        "- Then mention the top 2-3 most relevant products with key details.\n",
        "- End with a helpful suggestion or question if appropriate.\"\"\"\n",
        "\n",
        "\n",
        "def format_product_context(results: pd.DataFrame, max_products: int = 5) -> str:\n",
        "    \"\"\"Format retrieved products into a structured context string for the LLM.\"\"\"\n",
        "    if results.empty:\n",
        "        return \"No products found matching this query.\"\n",
        "\n",
        "    lines = []\n",
        "    for i, (_, row) in enumerate(results.head(max_products).iterrows()):\n",
        "        parts = [f\"Product {i+1}: {row.get('item_name_flat', 'Unknown Product')}\"]\n",
        "        parts.append(f\"  ID: {row.get('item_id', 'N/A')}\")\n",
        "\n",
        "        if pd.notna(row.get('brand_flat')) and str(row['brand_flat']).strip():\n",
        "            parts.append(f\"  Brand: {row['brand_flat']}\")\n",
        "        if pd.notna(row.get('product_type_flat')):\n",
        "            parts.append(f\"  Category: {row['product_type_flat']}\")\n",
        "        if pd.notna(row.get('color_flat')) and str(row['color_flat']).strip():\n",
        "            parts.append(f\"  Color: {row['color_flat']}\")\n",
        "        if pd.notna(row.get('price')):\n",
        "            parts.append(f\"  Price: ${row['price']:.2f}\")\n",
        "        if pd.notna(row.get('bullet_point_flat')):\n",
        "            bullets = str(row['bullet_point_flat'])[:400]\n",
        "            parts.append(f\"  Features: {bullets}\")\n",
        "        if pd.notna(row.get('image_caption')) and str(row['image_caption']).strip():\n",
        "            parts.append(f\"  Appearance: {row['image_caption']}\")\n",
        "\n",
        "        lines.append(\"\\n\".join(parts))\n",
        "\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "USER_TEMPLATE = \"\"\"Here are the top matching products from our catalog:\n",
        "\n",
        "{product_context}\n",
        "\n",
        "---\n",
        "Customer query: {query}\n",
        "\n",
        "Provide a helpful, concise recommendation based on the products above.\"\"\"\n",
        "\n",
        "\n",
        "# --- Quick test of context formatting ---\n",
        "test_ctx = format_product_context(test_results, max_products=3)\n",
        "print(\"Sample product context (3 products):\")\n",
        "print(\"=\" * 60)\n",
        "print(test_ctx)\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nContext length: {len(test_ctx)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6 – End-to-End RAG Pipeline\n",
        "\n",
        "The complete pipeline: `query → hybrid_search → context assembly → LLM generation → response`  \n",
        "With support for filters, session context (follow-ups), and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 6: End-to-End RAG Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def rag_query(\n",
        "    query: str,\n",
        "    llm = None,\n",
        "    llm_name: str = None,\n",
        "    top_k: int = 5,\n",
        "    price_max: float = None,\n",
        "    category: str = None,\n",
        "    session_history: List[Dict] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"End-to-end RAG pipeline: query -> search -> context -> LLM -> response.\n",
        "\n",
        "    Args:\n",
        "        query: Customer query text.\n",
        "        llm: LangChain LLM instance. Defaults to DEFAULT_LLM.\n",
        "        llm_name: Name for logging. Defaults to DEFAULT_LLM_NAME.\n",
        "        top_k: Number of products to retrieve.\n",
        "        price_max: Optional max price filter.\n",
        "        category: Optional category filter.\n",
        "        session_history: List of {role, content} dicts for multi-turn context.\n",
        "\n",
        "    Returns:\n",
        "        Dict with response_text, product_ids, products, status, timings.\n",
        "    \"\"\"\n",
        "    if llm is None:\n",
        "        llm = DEFAULT_LLM\n",
        "    if llm_name is None:\n",
        "        llm_name = DEFAULT_LLM_NAME\n",
        "\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"llm_name\": llm_name,\n",
        "        \"response_text\": \"\",\n",
        "        \"product_ids\": [],\n",
        "        \"status\": \"ok\",\n",
        "        \"retrieval_time\": 0.0,\n",
        "        \"generation_time\": 0.0,\n",
        "        \"total_time\": 0.0,\n",
        "    }\n",
        "\n",
        "    # --- Stage 1: Retrieve (uses ChromaDB if available, else in-memory) ---\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        search_results = search(\n",
        "            query, top_k=top_k, price_max=price_max, category=category\n",
        "        )\n",
        "    except Exception as e:\n",
        "        result[\"status\"] = \"pipeline_error\"\n",
        "        result[\"response_text\"] = \"Something went wrong with the search. Please try again.\"\n",
        "        result[\"retrieval_time\"] = time.time() - t0\n",
        "        return result\n",
        "\n",
        "    result[\"retrieval_time\"] = time.time() - t0\n",
        "\n",
        "    if search_results.empty:\n",
        "        result[\"status\"] = \"no_results\"\n",
        "        result[\"response_text\"] = (\n",
        "            \"No products match that query. Try different keywords or \"\n",
        "            \"broaden your filters.\"\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    # --- Stage 2: Build Context ---\n",
        "    product_context = format_product_context(search_results, max_products=top_k)\n",
        "    user_message = USER_TEMPLATE.format(\n",
        "        product_context=product_context, query=query\n",
        "    )\n",
        "\n",
        "    # --- Stage 3: Build Messages ---\n",
        "    messages = [SystemMessage(content=SYSTEM_PROMPT)]\n",
        "\n",
        "    # Add session history for follow-up context\n",
        "    if session_history:\n",
        "        for msg in session_history[-6:]:  # Last 3 turns (6 messages)\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                messages.append(HumanMessage(content=msg[\"content\"]))\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                messages.append(AIMessage(content=msg[\"content\"]))\n",
        "\n",
        "    messages.append(HumanMessage(content=user_message))\n",
        "\n",
        "    # --- Stage 4: Generate ---\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        response = llm.invoke(messages)\n",
        "        result[\"response_text\"] = response.content\n",
        "    except Exception as e:\n",
        "        result[\"status\"] = \"pipeline_error\"\n",
        "        result[\"response_text\"] = \"Something went wrong. Please try again.\"\n",
        "        result[\"generation_time\"] = time.time() - t1\n",
        "        result[\"total_time\"] = result[\"retrieval_time\"] + result[\"generation_time\"]\n",
        "        print(f\"\\u26a0 LLM Error: {e}\")\n",
        "        return result\n",
        "\n",
        "    result[\"generation_time\"] = time.time() - t1\n",
        "    result[\"total_time\"] = result[\"retrieval_time\"] + result[\"generation_time\"]\n",
        "    result[\"product_ids\"] = search_results[\"item_id\"].tolist()[:top_k]\n",
        "    result[\"products\"] = search_results.head(top_k)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"\\u2713 rag_query() defined — complete RAG pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 6b: Test the RAG Pipeline with Sample Queries\n",
        "# ============================================================\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    \"red shoes for women\",\n",
        "    \"3D printer filament PLA\",\n",
        "    \"phone case with colorful design\",\n",
        "    \"comfortable cotton t-shirt\",\n",
        "    \"kitchen drawer slides hardware\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RAG PIPELINE — DEMO QUERIES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "demo_results = []\n",
        "_LINE = \"─\" * 80\n",
        "\n",
        "for q in TEST_QUERIES:\n",
        "    print(f\"\\n{_LINE}\")\n",
        "    print(f\"\\u2753 Query: \\\"{q}\\\"\")\n",
        "    print(_LINE)\n",
        "\n",
        "    result = rag_query(q, top_k=3)\n",
        "    demo_results.append(result)\n",
        "\n",
        "    print(f\"\\n\\ud83d\\udcac Response ({result['llm_name']}):\")\n",
        "    print(f\"  {result['response_text']}\")\n",
        "    print(f\"\\n\\u23f1 Retrieval: {result['retrieval_time']:.3f}s | \"\n",
        "          f\"Generation: {result['generation_time']:.3f}s | \"\n",
        "          f\"Total: {result['total_time']:.3f}s\")\n",
        "    print(f\"  Products: {result['product_ids'][:3]}\")\n",
        "    print(f\"  Status: {result['status']}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(\"TIMING SUMMARY\")\n",
        "print(f\"{'=' * 80}\")\n",
        "retrieval_times = [r[\"retrieval_time\"] for r in demo_results]\n",
        "generation_times = [r[\"generation_time\"] for r in demo_results]\n",
        "total_times = [r[\"total_time\"] for r in demo_results]\n",
        "print(f\"  Retrieval:  avg={np.mean(retrieval_times):.3f}s  p95={np.percentile(retrieval_times, 95):.3f}s\")\n",
        "print(f\"  Generation: avg={np.mean(generation_times):.3f}s  p95={np.percentile(generation_times, 95):.3f}s\")\n",
        "print(f\"  Total:      avg={np.mean(total_times):.3f}s  p95={np.percentile(total_times, 95):.3f}s\")\n",
        "_pass_fail = \"✅ PASS\" if np.percentile(total_times, 95) < 5 else \"❌ FAIL\"\n",
        "print(f\"  Target: <5s total (per requirements.md) → {_pass_fail}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 6c: Multi-Turn Conversation Test\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MULTI-TURN CONVERSATION TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "session_history = []\n",
        "\n",
        "# Turn 1: Initial query\n",
        "q1 = \"Show me women's shoes\"\n",
        "print(f\"\\n\\ud83d\\udc64 Turn 1: \\\"{q1}\\\"\")\n",
        "r1 = rag_query(q1, top_k=3, session_history=session_history)\n",
        "print(f\"\\ud83e\\udd16 {r1['response_text']}\")\n",
        "session_history.append({\"role\": \"user\", \"content\": q1})\n",
        "session_history.append({\"role\": \"assistant\", \"content\": r1['response_text']})\n",
        "\n",
        "# Turn 2: Follow-up with context\n",
        "q2 = \"Do you have any in red?\"\n",
        "print(f\"\\n\\ud83d\\udc64 Turn 2: \\\"{q2}\\\"\")\n",
        "r2 = rag_query(\"red women's shoes\", top_k=3, session_history=session_history)\n",
        "print(f\"\\ud83e\\udd16 {r2['response_text']}\")\n",
        "session_history.append({\"role\": \"user\", \"content\": q2})\n",
        "session_history.append({\"role\": \"assistant\", \"content\": r2['response_text']})\n",
        "\n",
        "# Turn 3: Price filter\n",
        "q3 = \"Show me something under $50\"\n",
        "print(f\"\\n\\ud83d\\udc64 Turn 3: \\\"{q3}\\\"\")\n",
        "r3 = rag_query(\"red women's shoes\", top_k=3, price_max=50.0, session_history=session_history)\n",
        "print(f\"\\ud83e\\udd16 {r3['response_text']}\")\n",
        "\n",
        "print(f\"\\n\\u2713 Multi-turn conversation working with session context\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7 – Model Comparison (all available LLMs)\n",
        "\n",
        "Per `constitution.md`: *\"LLM: Comparative Study — Proprietary: GPT-4o. Open Source: Llama 3 (via Ollama or Groq).\"*  \n",
        "We compare **all available models** on the same queries for quality, latency, and response length.  \n",
        "Locally this may be Ollama vs OpenAI; on Kaggle it may be GPT-4o-mini vs Groq Llama-3.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 7: Model Comparison (GPT-4o-mini vs Llama-3.3-70B)\n",
        "# ============================================================\n",
        "\n",
        "COMPARISON_QUERIES = [\n",
        "    \"red shoes for women under $80\",\n",
        "    \"3D printer PLA filament\",\n",
        "    \"phone case with colorful design\",\n",
        "    \"comfortable cotton t-shirt for men\",\n",
        "    \"kitchen drawer slides\",\n",
        "    \"waterproof backpack\",\n",
        "    \"bedding set queen size\",\n",
        "    \"modern table lamp\",\n",
        "    \"stainless steel watch\",\n",
        "    \"leather ottoman for living room\",\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "print(\"Running model comparison...\\n\")\n",
        "for q in COMPARISON_QUERIES:\n",
        "    row = {\"query\": q}\n",
        "\n",
        "    for name, llm in llm_registry.items():\n",
        "        try:\n",
        "            result = rag_query(q, llm=llm, llm_name=name, top_k=3)\n",
        "            row[f\"{name}_response\"] = result[\"response_text\"]\n",
        "            row[f\"{name}_retrieval_s\"] = result[\"retrieval_time\"]\n",
        "            row[f\"{name}_generation_s\"] = result[\"generation_time\"]\n",
        "            row[f\"{name}_total_s\"] = result[\"total_time\"]\n",
        "            row[f\"{name}_status\"] = result[\"status\"]\n",
        "            row[f\"{name}_word_count\"] = len(result[\"response_text\"].split())\n",
        "        except Exception as e:\n",
        "            row[f\"{name}_response\"] = f\"ERROR: {e}\"\n",
        "            row[f\"{name}_status\"] = \"error\"\n",
        "\n",
        "    comparison_results.append(row)\n",
        "    print(f\"  \\u2713 {q[:50]}\")\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_results)\n",
        "\n",
        "# --- Summary Statistics ---\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(f\"{'=' * 80}\")\n",
        "\n",
        "for name in llm_registry:\n",
        "    gen_col = f\"{name}_generation_s\"\n",
        "    total_col = f\"{name}_total_s\"\n",
        "    wc_col = f\"{name}_word_count\"\n",
        "\n",
        "    if gen_col in df_comparison.columns:\n",
        "        gen_times = df_comparison[gen_col].dropna()\n",
        "        total_times = df_comparison[total_col].dropna()\n",
        "        word_counts = df_comparison[wc_col].dropna()\n",
        "\n",
        "        print(f\"\\n  {name}:\")\n",
        "        print(f\"    Generation: avg={gen_times.mean():.3f}s  p95={gen_times.quantile(0.95):.3f}s\")\n",
        "        print(f\"    Total:      avg={total_times.mean():.3f}s  p95={total_times.quantile(0.95):.3f}s\")\n",
        "        print(f\"    Words/resp: avg={word_counts.mean():.0f}  min={word_counts.min():.0f}  max={word_counts.max():.0f}\")\n",
        "        ok_count = (df_comparison[f\"{name}_status\"] == \"ok\").sum()\n",
        "        print(f\"    Success:    {ok_count}/{len(df_comparison)} queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 7b: Side-by-Side Response Comparison (First 3 queries)\n",
        "# ============================================================\n",
        "\n",
        "model_names = list(llm_registry.keys())\n",
        "\n",
        "for i, row in df_comparison.head(3).iterrows():\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Query: \\\"{row['query']}\\\"\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    for name in model_names:\n",
        "        resp_col = f\"{name}_response\"\n",
        "        gen_col = f\"{name}_generation_s\"\n",
        "        if resp_col in row:\n",
        "            print(f\"\\n  [{name}] ({row.get(gen_col, 0):.2f}s):\")\n",
        "            # Wrap long lines\n",
        "            text = str(row[resp_col])\n",
        "            for line in text.split(\"\\n\"):\n",
        "                print(f\"    {line}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 8 – Generation Quality Evaluation\n",
        "\n",
        "Evaluate the RAG pipeline on faithfulness, relevance, and completeness.  \n",
        "Per `test_strategy.md`: *\"Manually score 50 query responses for Helpfulness and Naturalness.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 8: Automated Quality Evaluation\n",
        "# ============================================================\n",
        "# We evaluate using LLM-as-judge (per RAGAS methodology):\n",
        "# - Faithfulness: Does the response only use info from context?\n",
        "# - Answer Relevance: Does the response address the query?\n",
        "# - Conciseness: Is the response appropriately concise?\n",
        "\n",
        "EVAL_PROMPT = \"\"\"You are an evaluation judge for a shopping assistant chatbot.\n",
        "Rate the following response on three criteria (score 1-5 each):\n",
        "\n",
        "1. FAITHFULNESS: Does the response ONLY mention products/details from the provided context?\n",
        "   5=perfect, 1=invents products/details\n",
        "2. RELEVANCE: Does the response address the customer's query well?\n",
        "   5=perfectly relevant, 1=completely off-topic\n",
        "3. HELPFULNESS: Is the response helpful, natural, and well-formatted?\n",
        "   5=excellent shopping advice, 1=unhelpful/robotic\n",
        "\n",
        "CUSTOMER QUERY: {query}\n",
        "\n",
        "RETRIEVED PRODUCTS (context):\n",
        "{context}\n",
        "\n",
        "ASSISTANT RESPONSE:\n",
        "{response}\n",
        "\n",
        "Return ONLY a JSON object with scores, no other text:\n",
        "{{\"faithfulness\": <1-5>, \"relevance\": <1-5>, \"helpfulness\": <1-5>, \"reasoning\": \"<brief explanation>\"}}\"\"\"\n",
        "\n",
        "\n",
        "# Extended evaluation queries (50 queries per test_strategy.md)\n",
        "EVAL_QUERIES = [\n",
        "    \"red shoes for women\",\n",
        "    \"3D printer filament PLA\",\n",
        "    \"phone case with colorful design\",\n",
        "    \"comfortable cotton t-shirt\",\n",
        "    \"kitchen drawer slides hardware\",\n",
        "    \"waterproof backpack for hiking\",\n",
        "    \"queen size bedding set\",\n",
        "    \"modern desk lamp LED\",\n",
        "    \"stainless steel men's watch\",\n",
        "    \"leather ottoman\",\n",
        "    \"blue running sneakers\",\n",
        "    \"iPhone case clear\",\n",
        "    \"bathroom towel set\",\n",
        "    \"wooden dining table\",\n",
        "    \"women's sandals summer\",\n",
        "    \"ABS filament black\",\n",
        "    \"kids' backpack school\",\n",
        "    \"decorative throw pillow\",\n",
        "    \"men's leather boots\",\n",
        "    \"Samsung Galaxy case\",\n",
        "    \"ergonomic office chair\",\n",
        "    \"kitchen cabinet handles\",\n",
        "    \"curtains for bedroom\",\n",
        "    \"white cotton polo shirt\",\n",
        "    \"ring gold plated\",\n",
        "    \"outdoor patio furniture\",\n",
        "    \"bathroom shelf hardware\",\n",
        "    \"women's winter boots\",\n",
        "    \"floor lamp living room\",\n",
        "    \"travel luggage carry-on\",\n",
        "    \"shower curtain waterproof\",\n",
        "    \"men's running shoes Nike\",\n",
        "    \"soft bed sheets queen\",\n",
        "    \"wall mount shelf bracket\",\n",
        "    \"colorful phone case floral\",\n",
        "    \"wooden coffee table modern\",\n",
        "    \"men's casual t-shirt\",\n",
        "    \"silver watch women's\",\n",
        "    \"kitchen faucet handle\",\n",
        "    \"decorative table lamp\",\n",
        "    \"hiking boots waterproof men\",\n",
        "    \"silk pillowcase set\",\n",
        "    \"phone case with kickstand\",\n",
        "    \"velvet ottoman round\",\n",
        "    \"drawer pull handles brass\",\n",
        "    \"women's athletic sneakers\",\n",
        "    \"3D printer nozzle\",\n",
        "    \"home storage basket\",\n",
        "    \"accent chair for bedroom\",\n",
        "    \"men's dress watch classic\",\n",
        "]\n",
        "\n",
        "\n",
        "def evaluate_response(query: str, context: str, response: str, judge_llm=None) -> dict:\n",
        "    \"\"\"Use LLM-as-judge to evaluate a RAG response.\"\"\"\n",
        "    if judge_llm is None:\n",
        "        judge_llm = DEFAULT_LLM\n",
        "\n",
        "    eval_message = EVAL_PROMPT.format(\n",
        "        query=query, context=context, response=response\n",
        "    )\n",
        "    try:\n",
        "        result = judge_llm.invoke([HumanMessage(content=eval_message)])\n",
        "        # Parse JSON from response\n",
        "        text = result.content.strip()\n",
        "        # Handle markdown code blocks\n",
        "        if \"```\" in text:\n",
        "            text = text.split(\"```\")[1]\n",
        "            if text.startswith(\"json\"):\n",
        "                text = text[4:]\n",
        "            text = text.strip()\n",
        "        scores = json.loads(text)\n",
        "        return scores\n",
        "    except Exception as e:\n",
        "        return {\"faithfulness\": 0, \"relevance\": 0, \"helpfulness\": 0, \"reasoning\": f\"Parse error: {e}\"}\n",
        "\n",
        "\n",
        "# --- Run evaluation on 50 queries ---\n",
        "print(f\"Running evaluation on {len(EVAL_QUERIES)} queries with {DEFAULT_LLM_NAME}...\\n\")\n",
        "\n",
        "eval_results = []\n",
        "\n",
        "for i, q in enumerate(EVAL_QUERIES):\n",
        "    # Generate response\n",
        "    rag_result = rag_query(q, top_k=3)\n",
        "\n",
        "    # Get context for evaluation\n",
        "    if \"products\" in rag_result and rag_result[\"products\"] is not None:\n",
        "        context = format_product_context(rag_result[\"products\"], max_products=3)\n",
        "    else:\n",
        "        context = \"No products found.\"\n",
        "\n",
        "    # Evaluate with LLM-as-judge\n",
        "    scores = evaluate_response(q, context, rag_result[\"response_text\"])\n",
        "\n",
        "    eval_results.append({\n",
        "        \"query\": q,\n",
        "        \"response_text\": rag_result[\"response_text\"],\n",
        "        \"product_ids\": str(rag_result[\"product_ids\"][:3]),\n",
        "        \"status\": rag_result[\"status\"],\n",
        "        \"retrieval_time\": rag_result[\"retrieval_time\"],\n",
        "        \"generation_time\": rag_result[\"generation_time\"],\n",
        "        \"total_time\": rag_result[\"total_time\"],\n",
        "        \"faithfulness\": scores.get(\"faithfulness\", 0),\n",
        "        \"relevance\": scores.get(\"relevance\", 0),\n",
        "        \"helpfulness\": scores.get(\"helpfulness\", 0),\n",
        "        \"reasoning\": scores.get(\"reasoning\", \"\"),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"  \\u2713 {i+1}/{len(EVAL_QUERIES)} queries evaluated\")\n",
        "\n",
        "df_eval = pd.DataFrame(eval_results)\n",
        "print(f\"\\n\\u2713 Evaluation complete: {len(df_eval)} queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 8b: Evaluation Summary & Metrics\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Quality scores\n",
        "for metric in [\"faithfulness\", \"relevance\", \"helpfulness\"]:\n",
        "    vals = df_eval[metric][df_eval[metric] > 0]  # Exclude parse failures\n",
        "    if len(vals) > 0:\n",
        "        print(f\"\\n  {metric.upper():15s}  mean={vals.mean():.2f}/5  \"\n",
        "              f\"median={vals.median():.1f}  min={vals.min():.0f}  max={vals.max():.0f}  \"\n",
        "              f\"(n={len(vals)})\")\n",
        "\n",
        "# Latency\n",
        "print(f\"\\n  LATENCY:\")\n",
        "print(f\"    Retrieval  — avg: {df_eval['retrieval_time'].mean():.3f}s  \"\n",
        "      f\"p95: {df_eval['retrieval_time'].quantile(0.95):.3f}s  \"\n",
        "      f\"p99: {df_eval['retrieval_time'].quantile(0.99):.3f}s\")\n",
        "print(f\"    Generation — avg: {df_eval['generation_time'].mean():.3f}s  \"\n",
        "      f\"p95: {df_eval['generation_time'].quantile(0.95):.3f}s  \"\n",
        "      f\"p99: {df_eval['generation_time'].quantile(0.99):.3f}s\")\n",
        "print(f\"    Total      — avg: {df_eval['total_time'].mean():.3f}s  \"\n",
        "      f\"p95: {df_eval['total_time'].quantile(0.95):.3f}s  \"\n",
        "      f\"p99: {df_eval['total_time'].quantile(0.99):.3f}s\")\n",
        "\n",
        "# Status distribution\n",
        "print(f\"\\n  STATUS:\")\n",
        "for status, count in df_eval['status'].value_counts().items():\n",
        "    print(f\"    {status}: {count}/{len(df_eval)}\")\n",
        "\n",
        "# Performance vs requirements\n",
        "p95_total = df_eval['total_time'].quantile(0.95)\n",
        "p99_total = df_eval['total_time'].quantile(0.99)\n",
        "print(f\"\\n  REQUIREMENTS CHECK:\")\n",
        "_ret_icon = \"✅\" if df_eval['retrieval_time'].quantile(0.95) < 1 else \"❌\"\n",
        "_tot_icon = \"✅\" if p95_total < 5 else \"❌\"\n",
        "print(f\"    RAG retrieval < 1s:   {_ret_icon}  \"\n",
        "      f\"(p95={df_eval['retrieval_time'].quantile(0.95):.3f}s)\")\n",
        "print(f\"    Total < 5s:           {_tot_icon}  (p95={p95_total:.3f}s)\")\n",
        "print(f\"    P99 total:            {p99_total:.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 9 – Export Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 9: Export Artifacts\n",
        "# ============================================================\n",
        "\n",
        "# 1. Products with prices\n",
        "products_path = EXPORT_DIR / \"products_with_prices.pkl\"\n",
        "df.to_pickle(products_path)\n",
        "print(f\"\\u2713 {products_path.name:35s}  {products_path.stat().st_size / 1e6:.1f} MB\")\n",
        "\n",
        "# Also save as CSV for portability\n",
        "csv_path = EXPORT_DIR / \"products_with_prices.csv\"\n",
        "export_cols = [\n",
        "    \"item_id\", \"item_name_flat\", \"brand_flat\", \"product_type_flat\",\n",
        "    \"color_flat\", \"price\", \"bullet_point_flat\", \"item_keywords_flat\",\n",
        "    \"image_caption\", \"enriched_text\", \"main_image_id\", \"path\", \"country\",\n",
        "]\n",
        "available_cols = [c for c in export_cols if c in df.columns]\n",
        "df[available_cols].to_csv(csv_path, index=False)\n",
        "print(f\"\\u2713 {csv_path.name:35s}  {csv_path.stat().st_size / 1e6:.1f} MB\")\n",
        "\n",
        "# 2. Evaluation results\n",
        "eval_path = EXPORT_DIR / \"llm_evaluation.csv\"\n",
        "df_eval.to_csv(eval_path, index=False)\n",
        "print(f\"\\u2713 {eval_path.name:35s}  {eval_path.stat().st_size / 1e3:.1f} KB\")\n",
        "\n",
        "# 3. Model comparison results\n",
        "comp_path = EXPORT_DIR / \"llm_comparison.csv\"\n",
        "df_comparison.to_csv(comp_path, index=False)\n",
        "print(f\"\\u2713 {comp_path.name:35s}  {comp_path.stat().st_size / 1e3:.1f} KB\")\n",
        "\n",
        "# 4. LLM config\n",
        "llm_config = {\n",
        "    \"notebook\": \"04-llm-integration\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"default_llm\": DEFAULT_LLM_NAME,\n",
        "    \"available_llms\": list(llm_registry.keys()),\n",
        "    \"system_prompt\": SYSTEM_PROMPT,\n",
        "    \"user_template\": USER_TEMPLATE,\n",
        "    \"rag_config\": RAG_CONFIG,\n",
        "    \"evaluation_summary\": {\n",
        "        \"n_queries\": len(df_eval),\n",
        "        \"faithfulness_mean\": round(float(df_eval[\"faithfulness\"].mean()), 2),\n",
        "        \"relevance_mean\": round(float(df_eval[\"relevance\"].mean()), 2),\n",
        "        \"helpfulness_mean\": round(float(df_eval[\"helpfulness\"].mean()), 2),\n",
        "        \"retrieval_p95_s\": round(float(df_eval[\"retrieval_time\"].quantile(0.95)), 3),\n",
        "        \"generation_p95_s\": round(float(df_eval[\"generation_time\"].quantile(0.95)), 3),\n",
        "        \"total_p95_s\": round(float(df_eval[\"total_time\"].quantile(0.95)), 3),\n",
        "    },\n",
        "    \"price_ranges\": CATEGORY_PRICE_RANGES,\n",
        "    \"device\": str(DEVICE),\n",
        "    \"gpu_name\": GPU_NAME,\n",
        "}\n",
        "\n",
        "config_path = EXPORT_DIR / \"llm_config.json\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(llm_config, f, indent=2, default=str)\n",
        "print(f\"\\u2713 {config_path.name:35s}  {config_path.stat().st_size / 1e3:.1f} KB\")\n",
        "\n",
        "print(f\"\\n\\u2713 All artifacts exported to {EXPORT_DIR}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Final Summary\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SHOPTALK LLM INTEGRATION \\u2014 SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n--- Pipeline ---\")\n",
        "print(f\"  Products:        {len(df):,}\")\n",
        "print(f\"  Search:          Hybrid (text + image) with production reranking\")\n",
        "print(f\"  Default LLM:     {DEFAULT_LLM_NAME}\")\n",
        "print(f\"  Available LLMs:  {list(llm_registry.keys())}\")\n",
        "print(f\"  Device:          {DEVICE} ({GPU_NAME})\")\n",
        "\n",
        "print(\"\\n--- Quality (LLM-as-Judge, 50 queries) ---\")\n",
        "for m in [\"faithfulness\", \"relevance\", \"helpfulness\"]:\n",
        "    v = df_eval[m][df_eval[m] > 0]\n",
        "    if len(v): print(f\"  {m:15s}  {v.mean():.2f}/5\")\n",
        "\n",
        "print(\"\\n--- Latency ---\")\n",
        "print(f\"  Retrieval p95:   {df_eval['retrieval_time'].quantile(0.95):.3f}s  (target < 1s)\")\n",
        "print(f\"  Total p95:       {df_eval['total_time'].quantile(0.95):.3f}s  (target < 5s)\")\n",
        "print(f\"  Total p99:       {df_eval['total_time'].quantile(0.99):.3f}s\")\n",
        "\n",
        "print(\"\\n--- Exports ---\")\n",
        "for p in sorted(EXPORT_DIR.glob(\"*\")):\n",
        "    if p.is_file() and p.suffix in [\".pkl\", \".csv\", \".json\", \".npy\"]:\n",
        "        print(f\"  {p.name:35s}  {p.stat().st_size / 1e6:>6.1f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LLM integration complete. Next: 05-fine-tuning.ipynb\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
